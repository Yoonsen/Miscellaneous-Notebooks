{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\larsj\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.26.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as PyP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        pdf = PdfFileReader(f)\n",
    "        info = pdf.getDocumentInfo()\n",
    "        number_of_pages = pdf.getNumPages()\n",
    "    print(number_of_pages)\n",
    "\n",
    "\n",
    "def get_text_from_pdf(path):\n",
    "    text = \"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        pdf = PdfFileReader(f)\n",
    "        number_of_pages = pdf.getNumPages()\n",
    "        for p in range(number_of_pages):\n",
    "            page = pdf.getPage(p)\n",
    "            text += page.extractText()\n",
    "    return text\n",
    "\n",
    "def files(path):\n",
    "    import os\n",
    "    r, d, f = next(os.walk(path))\n",
    "    pdfer = [file for file in f if file.endswith('.pdf')]\n",
    "    return pdfer\n",
    "\n",
    "def get_text_from_word(document):\n",
    "    \"\"\"Find all text in a Word document\"\"\"\n",
    "    import sys\n",
    "    import zipfile\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    with zipfile.ZipFile(document, 'r') as zfp:\n",
    "        with zfp.open('word/document.xml') as fp:\n",
    "            soup = BeautifulSoup(fp.read(), 'xml')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gswin32c.exe ^\n",
    "  -o repaired.pdf ^\n",
    "  -sDEVICE=pdfwrite ^\n",
    "  -dPDFSETTINGS=/prepress ^\n",
    "   corrupted.pdf\n",
    "    \n",
    "    \n",
    "    print(pageObj.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is System\n",
      " Volume Serial Number is D4A3-F5F9\n",
      "\n",
      " Directory of C:\\Users\\larsj\\Documents\\GitHub\\Miscellaneous-Notebooks\n",
      "\n",
      "14.03.2019  20:15            33ÿ359 dhn2019-tutorial.pdf\n",
      "               1 File(s)         33ÿ359 bytes\n",
      "               0 Dir(s)  44ÿ371ÿ726ÿ336 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir *.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining digital libraries\n",
      " \n",
      "The central objective\n",
      " \n",
      "of\n",
      " \n",
      "this \n",
      "tutorial\n",
      " \n",
      "is \n",
      "to show how researcher\n",
      "s\n",
      " \n",
      "within the\n",
      " \n",
      "humanities\n",
      " \n",
      "can\n",
      " \n",
      "ac-\n",
      "cess\n",
      " \n",
      "and \n",
      "use\n",
      " \n",
      "a\n",
      " \n",
      "cloud\n",
      "-\n",
      "based\n",
      " \n",
      "corpus\n",
      " \n",
      "from\n",
      " \n",
      "within \n",
      "Jupyter Notebook, exemplified\n",
      " \n",
      "with\n",
      " \n",
      "the digital \n",
      "collections of the Norwegian national library.\n",
      " \n",
      "The tutorial \n",
      "is\n",
      " \n",
      "relevant for the Nordic humanistic \n",
      "community in several ways:\n",
      " \n",
      "1)\n",
      " \n",
      "Show how copyrighted material can be \n",
      "utilized for corpus studies\n",
      ". \n",
      " \n",
      "2)\n",
      " \n",
      "Differe\n",
      "nt tools built as modules\n",
      " \n",
      "over an API for\n",
      " \n",
      "defining and accessing a \n",
      "corpus \n",
      "for \n",
      "analysi\n",
      "s\n",
      " \n",
      "like\n",
      " \n",
      "character modelling, collocation analysis, clustering, growth diagrams and \n",
      "more.\n",
      " \n",
      "3)\n",
      " \n",
      "Demonstrate the benefits of \n",
      "using \n",
      "Jupyter Notebook for researchers without program-\n",
      "ming background.\n",
      " \n",
      "4)\n",
      " \n",
      "Study the\n",
      " \n",
      "connection between texts and library metadata\n",
      " \n",
      "As for (1\n",
      ")\n",
      ", a \n",
      "problem for many researchers is the use of \n",
      "copyrighted material. However, the \n",
      "researcher \n",
      "often \n",
      "has \n",
      "no need for the\n",
      " \n",
      "actual text as \n",
      "such;\n",
      " \n",
      "some features of it\n",
      " \n",
      "may suffice, \n",
      "like \n",
      "bag\n",
      " \n",
      "of words, \n",
      "a\n",
      " \n",
      "participle count\n",
      " \n",
      "or a character model. None of these features challenge the \n",
      "copyright\n",
      " \n",
      "holder. A centralized repository of copyrighted material can provide feature sets.\n",
      " \n",
      "The po\n",
      "ints in (2) and (3) cover\n",
      " \n",
      "the need for programmers\n",
      " \n",
      "as well as\n",
      " \n",
      "researchers without pro-\n",
      "gramming background. \n",
      "While the former need\n",
      " \n",
      "a docume\n",
      "ntation of the \n",
      "low\n",
      "-\n",
      "level\n",
      " \n",
      "interf\n",
      "ace to \n",
      "the cloud, the actual API, t\n",
      "he latter want\n",
      " \n",
      "an accessible interface for doing corpus analysis\n",
      ", and \n",
      "Jupyter Notebook provides such an interface\n",
      " \n",
      "via top level functions and commands expressed \n",
      "in a programming language, e.g. Python or R\n",
      ". \n",
      " \n",
      "Item (4) shows how \n",
      "readymade\n",
      " \n",
      "library metadata can be integrated for building corpora based \n",
      "on those data, \n",
      "like Dewey \n",
      "decimal codes or topic words. \n",
      "We will show how m\n",
      "etadata can be \n",
      "used to build, select and compare corpora.\n",
      " \n",
      "The participants will\n",
      " \n",
      "experimen\n",
      "t \n",
      "with the API, \n",
      "and \n",
      "get a\n",
      " \n",
      "hands on experience with the tools.\n",
      " \n",
      "The kind of material we go through will be distributed as a Github repository\n",
      " \n",
      "on format similar \n",
      "to\n",
      " \n",
      "th\n",
      "is\n",
      ": \n",
      "https://github.com/Yoonsen/NB_API_Python\n",
      ".\n",
      " \n",
      "Background\n",
      " \n",
      "At the National Library of Norway, a mass digi\n",
      "tization project was initiated in 2006, with the \n",
      "goal of digitizing the entire collection of books, newspapers, movies, radio\n",
      "-\n",
      " \n",
      "and television\n",
      "-\n",
      "broadcasts, music etc., in sum everything published in the public domain in Norway of all media \n",
      "types, the entire \n",
      "cultural heritage of Norway. For the books, \n",
      "the entire stock \n",
      "was finished dig-\n",
      "itized in 2017, a collection of\n",
      " \n",
      "about \n",
      "500.000 books\n",
      ". \n",
      " \n",
      "The corpus \n",
      "that is \n",
      "available via the API, consists of about 50 billion tokens, which is considered \n",
      "big for a \n",
      "rather small language like Norwegian (5 million speakers). In comparison, the Google \n",
      "Books corpus contains approximately 500 billion tokens for English.\n",
      " \n",
      "The National Library cooperates with scholars of literary studies and linguistics in developing \n",
      "and app\n",
      "lying methods of data mining to our material. We develop services that make our con-\n",
      "tent available for quantitative research, without challenging intellectual property rights. One \n",
      "such service is NB N\n",
      "-\n",
      "gram for Norwegian, comparable to Google N\n",
      "-\n",
      "gram Viewer \n",
      "fo\n",
      "r English \n",
      "and other languages, as found here: \n",
      "http://www.nb.no/sp_tjenester/beta/ngram_1/\n",
      ".\n",
      ".\n",
      " \n",
      "Session Format\n",
      " \n",
      "Full day tutorial.\n",
      " \n",
      "We start out with an introduction \n",
      "to Notebooks using\n",
      " \n",
      "a motivating example \n",
      "with\n",
      " \n",
      "commands expressed as Python functions.\n",
      " \n",
      "We then move on to show how to \n",
      "build cor-\n",
      "pora using metadata, either from the libr\n",
      "ary \n",
      "i\n",
      "tself\n",
      " \n",
      "or within notebooks\n",
      ", and exemplify \n",
      "a corpus \n",
      "investigation by finding \n",
      "conco\n",
      "rdances (KWIC)\n",
      ". \n",
      "This part of the tutorial will al\n",
      "so cover \n",
      "issues\n",
      " \n",
      "concerning OCR, and how they might be\n",
      " \n",
      "overcome\n",
      " \n",
      "in searching. \n",
      "The \n",
      "next\n",
      " \n",
      "topics are on com-\n",
      "paring corpor\n",
      "a with respect to statistics of terminology, for example, what\n",
      " \n",
      "are the key words \n",
      "of Dewey 641, or texts written in a certain period. Then\n",
      " \n",
      "we \n",
      "plan on \n",
      "cover\n",
      "ing\n",
      " \n",
      "collocation anal-\n",
      "ysis and \n",
      "clustering\n",
      ".\n",
      " \n",
      "T\n",
      "hese topics are \n",
      "exemplified with notebooks in the\n",
      " \n",
      "Github\n",
      " \n",
      "repository\n",
      " \n",
      "mentioned above.\n",
      " \n",
      "Target \n",
      "audience\n",
      " \n",
      "and number of participants\n",
      " \n",
      "Humanities researchers, for example\n",
      " \n",
      "scholars of literary\n",
      " \n",
      "and media\n",
      " \n",
      "studies, corpus and com-\n",
      "putational linguists\n",
      ", as well as l\n",
      "ibrarians\n",
      " \n",
      "and curators.\n",
      " \n",
      "Number of participants should not exceed twenty.\n",
      " \n",
      "Technical requirements\n",
      " \n",
      "We will\n",
      " \n",
      "need projector and screen. P\n",
      "articipants \n",
      "bring their own laptops. \n",
      " \n",
      "Workshop \n",
      "leaders\n",
      " \n",
      "\n",
      " \n",
      "Lars G.\n",
      " \n",
      "B.\n",
      " \n",
      "Johnsen: Research librarian at the Nation\n",
      "al\n",
      " \n",
      "Library of Norway, PhD in lin-\n",
      "guistics. Fields of interest: semantics, grammar, philosophy of language, probability \n",
      "theory a\n",
      "nd applications. \n",
      "Email:\n",
      " \n",
      "l\n",
      "ars.\n",
      "j\n",
      "ohnsen@nb.no\n",
      " \n",
      "\n",
      " \n",
      "Yngvil Beyer\n",
      ": Research librarian at the National Library of Norway, \n",
      "Master in Media \n",
      "studies\n",
      ". Fields of interest:\n",
      " \n",
      "Manuscripts and\n",
      " \n",
      "media\n",
      ", handwritten text recognition, text \n",
      "enco\n",
      "d\n",
      "ing\n",
      ". \n",
      "Email:\n",
      " \n",
      "yngvil.beyer@nb.no\n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_text_from_pdf(\"dhn2019-tutorial.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_text_from_word() missing 1 required positional argument: 'document'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a77f02f24f5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_text_from_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_text_from_word() missing 1 required positional argument: 'document'"
     ]
    }
   ],
   "source": [
    "get_text_from_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    " \n",
    " \n",
    "def text_extractor(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        pdf = PdfFileReader(f)\n",
    " \n",
    "        # get the first page\n",
    "        page = pdf.getPage(1)\n",
    "        print(page)\n",
    "        #print('Page type: {}'.format(str(type(page))))\n",
    " \n",
    "        #text = page.extractText()\n",
    "        #print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extractor(\"../Semantikk semesteroppgaver 2018.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

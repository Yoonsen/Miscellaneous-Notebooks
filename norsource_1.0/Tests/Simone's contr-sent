9405001.xml:<A-S ID='A-3' AZ='CTR'> However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus . </A-S>
9405001.xml:<S ID='S-20' AZ='CTR'> Their model , however , is not probabilistic , that is , it does not provide a probability estimate for unobserved cooccurrences . </S>
9405001.xml:<S ID='S-21' AZ='CTR'> It cannot therefore be used in a complete probabilistic framework , such as n-gram language models or probabilistic lexicalized grammars <REF  TYPE='P'>Schabes 1992</REF>, <REF  TYPE='P'>Lafferty et al. 1992</REF> . </S>
9405001.xml:<S ID='S-35' AZ='CTR'> Because of data sparseness , we cannot reliably use a maximum likelihood estimator ( MLE ) for bigram probabilities . </S>
9405001.xml:<S ID='S-38' AZ='CTR'> However , this estimates the probability of any unseen bigram to be zero , which is clearly undesirable . </S>
9405001.xml:<S ID='S-115' AZ='CTR'> We will outline here the main parallels and differences between our method and cooccurrence smoothing . </S>
9405001.xml:<S ID='S-122' AZ='CTR'> Notice that this formula has the same form as our similarity model <CREF/> , except that it uses confusion probabilities where we use normalized weights . </S>
9405001.xml:<S ID='S-123' AZ='CTR'> In addition , we restrict the summation to sufficiently similar words , whereas the cooccurrence smoothing method sums over all words in the lexicon . </S>
9405001.xml:<S ID='S-127' AZ='CTR'> Therefore , in our similarity model w ' will play a stronger role in estimating w than vice versa . </S>
9405001.xml:<S ID='S-129' AZ='CTR'> Finally , while we have used our similarity model only for missing bigrams in a back-off scheme , <REF TYPE='A'>Essen and Steinbiss 1992</REF> used linear interpolation for all bigrams to combine the cooccurrence smoothing model with MLE models of bigrams and unigrams . </S>
9405002.xml:<S ID='S-11' AZ='CTR'> We address two related questions ; the first arises from treating the simple past as anaphoric . </S>
9405002.xml:<S ID='S-12' AZ='CTR'> Specifically , if a treatment such as <REFAUTHOR>Hinrichs</REFAUTHOR> 's is used to explain the forward progression of time in example <CREF/> , then it must be explained why sentence <CREF/> is as felicitous as sentence <CREF/> . </S>
9405002.xml:<S ID='S-13' AZ='CTR'> That is , one would predict a clash of temporal relations for sentence <CREF/> , since the simple pasts induce the forward progression of time but the conjunction indicates the reverse temporal ordering . </S>
9405002.xml:<S ID='S-14' AZ='CTR'> The second question arises from assuming that all temporal relations are recovered solely from reasoning with coherence relations . </S>
9405002.xml:<S ID='S-15' AZ='CTR'> Specifically , because the use of the simple past in passage <CREF/> is as felicitous as the past perfect in passage <CREF/> under the explanation interpretation ( in these cases indicated explicitly by because ) , then it must be explained why passage <CREF/> is not understood as an explanation as is passage <CREF/> , where in each case the relationship needs to be inferred . </S>
9405002.xml:<S ID='S-48' AZ='CTR'> Restating the first problem noted in Section <CREF/> , if treating the simple past as anaphoric is used to account for the forward progression of time in passage ( <CREF/> , then one would expect the existence of the Explanation relation in passage <CREF/> to cause a temporal clash , where in fact passage <CREF/> is perfectly felicitous . </S>
9405002.xml:<S ID='S-52' AZ='CTR'> Restating the second problem noted in Section <CREF/> , if temporal relations can be recovered solely from reasoning with coherence relations , and the use of the simple past in passage <CREF/> is as felicitous as the past perfect in passage <CREF/> under the Explanation interpretation , then one asks why passage <CREF/> is not understood as an Explanation as is passage <CREF/> , where in each case the relationship needs to be inferred . </S>
9405002.xml:<S ID='S-56' AZ='CTR'> We see several advantages of this approach over that of <REFAUTHOR>Lascarides and Asher</REFAUTHOR>, henceforth <REFAUTHOR>L and A</REFAUTHOR> . </S>
9405002.xml:<S ID='S-63' AZ='CTR'> However , this subset contains just those relations that are predicted to be possible by accounts treating the past perfect as anaphoric ; they are the ones that do not constrain the temporal order of the events against displaying backward progression of time . </S>
9405002.xml:<S ID='S-64' AZ='CTR'> Therefore , we see no advantages to adopting their rule ; furthermore , they do not comment on what other laws have to be stipulated to account for the facts concerning other possible tense combinations . </S>
9405002.xml:<S ID='S-67' AZ='CTR'> However , this does not explain why the use of the simple past is perfectly coherent when the Explanation relationship is indicated overtly as it is in sentence <CREF/> , nor do they adequately explain why CCT must be satisfied for this causal law and not for those supporting similar examples for which they successfully infer an unsignaled Explanation relation ( see discussion of example <CREF/> , pg. 463 ) . </S>
9405002.xml:<S ID='S-68' AZ='CTR'> Third , the <REFAUTHOR>Lascarides and Asher</REFAUTHOR> account does not explain why the past perfect cannot stand alone nor discourses generally be opened with it ; consider stating sentence <CREF/> in isolation : </S>
9405002.xml:<S ID='S-69' AZ='CTR'> Intuitively , such usage is infelicitous because of a dependency on a contextually salient time which has not been previously introduced . </S>
9405002.xml:<S ID='S-70' AZ='CTR'> This is not captured by the <REFAUTHOR>Lascarides and Asher</REFAUTHOR> account because sentences containing the past perfect are treated as sententially equivalent to those containing the simple past . </S>
9405002.xml:<S ID='S-71' AZ='CTR'> On the other hand , sentences in the simple past are perfectly felicitous in standing alone or opening a discourse , introducing an asymmetry in accounts treating the simple past as anaphoric to a previously evoked time . </S>
9405004.xml:<A-S ID='A-0' DOCUMENTC='S-110' AZ='CTR'> The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in <REF TYPE='P'>Shieber et al. 1990</REF> . </A-S>
9405004.xml:<S ID='S-3' AZ='CTR'> However , there are composition rules for semantic representations which violate this restriction , e.g. the schemata for the construction of Underspecified Discourse Representation Structures ( UDRSs ) <REF TYPE='P'>Frank and Reyle 1992</REF> where , in general , the root of a tree is associated with a strictly larger semantic structure than any of the leaves . </S>
9405004.xml:<S ID='S-26' AZ='CTR'> A strict left-to-right , depth-first expansion of subgoals might run into problems with the grammar fragment in <CREF/> if a left-recursive np-rule exists , because the semantics of the np is only instantiated once the 'semantic head ' of the vp has been looked up in the lexicon . </S>
9405004.xml:<S ID='S-29' AZ='CTR'> But there will be no helpful subgoal reordering for rules with semantic head recursion : </S>
9405004.xml:<S ID='S-37' AZ='CTR'> However , the lack of top-down guidance will lead , in general , to a lot of non-determinism . </S>
9405004.xml:<S ID='S-38' AZ='CTR'> The strong substructure condition means that the algorithm will be incomplete for grammars which cover semantically void phrases like expletive expressions , particles , and subphrases of idioms . </S>
9405004.xml:<S ID='S-49' AZ='CTR'> Note that <REFAUTHOR>van Noord</REFAUTHOR> 's method is restricted to grammars where phrases have always a lexical semantic head . </S>
9405004.xml:<S ID='S-51' AZ='CTR'> In the following , we will present shortly a semantic representation formalism and a corresponding set of analysis rules which resist to the definition of ` semantic head ' as it is required in <REFAUTHOR>van Noord</REFAUTHOR> 's head-corner algorithm . </S>
9405004.xml:<S ID='S-59' AZ='CTR'> Hence , the head-corner generator of Fig. <CREF/> with a link relation based on semantic heads will not be applicable . </S>
9405004.xml:<S ID='S-81' AZ='CTR'> However , if verb second configurations are described by a single structure </S>
9405004.xml:<S ID='S-82' AZ='CTR'> the algorithm runs into a deadlock : the vp-node cannot be processed completely , because the semantics of the XP-trace is unknown , and the expansion of the XP-filler position will be delayed for the same reason . </S>
9405010.xml:<S ID='S-1' AZ='CTR'> However , the conditions under which a representation of an utterance may serve as a suitable basis for interpreting subsequent elliptical forms remain poorly understood ; specifically , past attempts to characterize these processes within a single traditional module of language processing ( e.g. , considering either syntax , semantics , or discourse in isolation ) have failed to account for all of the data . </S>
9405010.xml:<S ID='S-17' AZ='CTR'> The common stipulation within the literature stating that gapping applies to coordinate structures and not to subordinate ones does not account for why any coordinated cases are unacceptable . </S>
9405010.xml:<S ID='S-24' AZ='CTR'> Past approaches to VP-ellipsis that operate within a single module of language processing fail to make the distinctions necessary to account for these differences . </S>
9405010.xml:<S ID='S-110' AZ='CTR'> The analysis accounts for the range of data given in <REF TYPE='A' SELF="YES">Kehler 1993b</REF> , although one point of departure exists between that account and the current one with respect to clauses conjoined with but . </S>
9405010.xml:<S ID='S-118' AZ='CTR'> <REFAUTHOR>Sag</REFAUTHOR> then suggests a weakening of his condition , with the result that both of the above examples are incorrectly predicted to be acceptable ; he doesn't consider a solution predicting the judgements as stated . </S>
9405010.xml:<S ID='S-135' AZ='CTR'> The current work improves upon that analysis in several respects . </S>
9405010.xml:<S ID='S-136' AZ='CTR'> First , it no longer needs to be posited that syntactic representations disappear when integrated into the discourse model ; instead , syntactic and semantic representations co-exist . </S>
9405010.xml:<S ID='S-137' AZ='CTR'> Second , various issues with regard to the interpretation of propositional representations are now rendered moot . </S>
9405010.xml:<S ID='S-138' AZ='CTR'> Third , there is no longer a dichotomy with respect to the level of representation from which VP-ellipsis locates and copies antecedents . </S>
9405010.xml:<S ID='S-140' AZ='CTR'> Finally , the current approach more readily scales up to more complex cases . </S>
9405010.xml:<S ID='S-141' AZ='CTR'> For instance , it was not clear in the previous account how non-parallel constructions embedded within parallel constructions would be handled , as in sentences <CREF/> : </S>
9405010.xml:<S ID='S-146' AZ='CTR'> This constraint rules out cases where VP-ellipsis obtains syntactically mismatched antecedents , such as example <CREF/> and other non-parallel cases given in <REF SELF="YES"
9405010.xml:<S ID='S-147' AZ='CTR'> It also appears that neither approach can account for the infelicity of mixed gapping / VP-ellipsis cases such as sentence <CREF/> . </S>
9405013.xml:<S ID='S-12' AZ='CTR'> This type of reference is different from the type that has been studied traditionally by researchers who have usually assumed that the agents have mutual knowledge of the referent <REF TYPE='P'>Appelt 1985a</REF> , <REF TYPE='P'>Appelt and Kronfeld 1987</REF> , <REF TYPE='P'>Clark and Wilkes-Gibbs 1986</REF> , <REF TYPE='P'>Heeman and Hirst 1992</REF> , <REF TYPE='P'>Searle 1969</REF> , are copresent with the referent <REF TYPE='P'>Heeman and Hirst 1992</REF> , <REF TYPE='P'>Cohen 1981</REF> , or have the referent in their focus of attention <REF TYPE='P'>Reiter and Dale 1992</REF> . </S>
9405013.xml:<S ID='S-50' AZ='CTR'> Obviously , an agent cannot use this criterion to understand the reference to the building in Example <CREF/> -- he has never heard of the building before . </S>
9405013.xml:<S ID='S-67' AZ='CTR'> But , ideally , salience should depend on the context surrounding the referent . </S>
9405013.xml:<S ID='S-68' AZ='CTR'> For example , the height of a tall building would normally be salient , but not if it were surrounded by other tall buildings . </S>
9405022.xml:<A-S ID='A-2' AZ='CTR' DOCUMENTC='S-19;S-18'> Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar . </A-S>
9405022.xml:<S ID='S-17' AZ='CTR'> A problem not fully explored yet is how to arrive at an optimal choice of tree-cutting criteria . </S>
9405022.xml:<S ID='S-18' AZ='CTR'> In the previous scheme , these must be specified manually , and the choice is left to the designer 's intuitions . </S>
9405023.xml:<S ID='S-25' AZ='CTR'> However , in many cases there are several distinct maximal parses , each consisting of a different subset of words of the original sentence . </S>
9405023.xml:<S ID='S-26' AZ='CTR'> Furthermore , our experience has shown that in many cases , ignoring an additional one or two input words may result in a parse that is syntactically and / or semantically more coherent . </S>
9405028.xml:<S ID='S-26' AZ='CTR'> This result is neither interpretation <CREF/> nor <CREF/> shown above . </S>
9405028.xml:<S ID='S-29' AZ='CTR'> Therefore the property sharing theory also fails to account for the intuitive interpretations . </S>
9405028.xml:<S ID='S-59' AZ='CTR'> However , in practice , the more the level of depth of recursively appearing ` Comment ' is , the less comprehensible the sentence is . </S>
9405028.xml:<S ID='S-76' AZ='CTR'> Also in <REF TYPE='A'>Palmer 1986</REF> its semantics is informally explained , however our proposal is to formalize garu 's semantics in UG or more generally in computational linguistics . </S>
9405028.xml:<S ID='S-78' AZ='CTR'> Although this notion of observer shares a large part with PIVOT of <REF TYPE='A'>Iida-Sells 1988</REF> , our notion of observer is introduced only by garu . </S>
9405028.xml:<S ID='S-79' AZ='CTR'> Therefore it is much narrower notion . </S>
9405028.xml:<S ID='S-179' AZ='CTR'> Our focus is a more pragmatics oriented one than JPSG is . </S>
9405028.xml:<S ID='S-187' AZ='CTR'> So what is new ? </S>
9405028.xml:<S ID='S-188' AZ='CTR'> The answer is that : </S>
9405028.xml:<S ID='S-192' AZ='CTR'> We formalize this theory in UG formalism , even though the details are omitted due to the space limitation . </S>
9405028.xml:<S ID='S-193' AZ='CTR'> We find that the constraints of complex sentences are actually local ones . </S>
9405033.xml:<S ID='S-4' AZ='CTR'> However , although the practical throughput of parsers with such realistic grammars is important , for example when processing large amounts of text or in interactive applications , there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars . </S>
9405033.xml:<S ID='S-5' AZ='CTR'> Previous comparisons have either focussed on context-free ( CF ) or augmented CF parsing <REF TYPE='P'>Tomita 1987</REF> , <REF TYPE='P'>Billot and Lang 1989</REF> , or have used relatively small , limited-coverage unification grammars and lexicons <REF TYPE='P'>Shann 1989</REF> , <REF TYPE='P'>Bouma and van Noord 1993</REF> , <REF TYPE='P'>Maxwell and Kaplan 1993</REF> . </S>
9405033.xml:<S ID='S-6' AZ='CTR'> It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic , complex unification-based grammars : in particular , with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length , and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques . </S>
9405033.xml:<S ID='S-7' AZ='CTR'> Also , since none of these studies relate observed performance to that of other comparable parsing systems , implementational oversights may not be apparent and so be a confounding factor in any general conclusions made . </S>
9405033.xml:<S ID='S-9' AZ='CTR'> However , parsing algorithms assume more importance for grammars having more substantial phrase structure components , such as CLARE ( which although employing some HPSG-like analyses still contains several tens of rules ) and the ANLT ( which uses a formalism derived from GPSG ) <REF TYPE='P'>Gazdar et al. 1985</REF> , since the more specific rule set can be used to control which unifications are performed . </S>
9405033.xml:<S ID='S-45' AZ='CTR'> The grammar-dependent complexity of the LR parser makes it also appear intractable : <REF TYPE='A'>Johnson 1989</REF> shows that the number of LR(0) states for certain ( pathological ) grammars is exponentially related to the size of the grammar , and that there are some inputs which force an LR parser to visit all of these states in the course of a parse . </S>
9405033.xml:<S ID='S-46' AZ='CTR'> Thus the total number of operations performed , and also space consumed ( by the vertices in the graph-structured stack ) , is an exponential function of the size of the grammar . </S>
9405033.xml:<S ID='S-101' AZ='CTR'> In previous work with the ANLT <REF TYPE='P' SELF="YES">Briscoe and Carroll 1993</REF> , throughput with raw corpus data was worse than that observed in these experiments , though probably only by a constant factor . </S>
9405035.xml:<S ID='S-30' AZ='CTR'> This engineering approach requires great effort in designing the representation and the mapping rules . </S>
9405035.xml:<S ID='S-36' AZ='CTR'> However , the accuracy is low . </S>
9407011.xml:<S ID='S-4' AZ='CTR'> This approach has many strong points , but does not provide a very satisfactory account of the adherence to discourse conventions in dialogue . </S>
9407011.xml:<S ID='S-15' AZ='CTR'> While these accounts do help explain some discourse phenomena more satisfactorily , they still require a strong degree of co-operativity to account for dialogue coherence , and do not provide easy explanations of why an agent might act in cases that do not support high-level mutual goals . </S>
9407011.xml:<S ID='S-26' AZ='CTR'> Games provide a better explanation of coherence , but still require the agents to recognize each other 's intentions to perform the dialogue game . </S>
9407011.xml:<S ID='S-29' AZ='CTR'> Because of this separation , they do not have to assume co-operation on the tasks each agent is performing , but still require recognition of intention and co-operation at the conversational level . </S>
9407011.xml:<S ID='S-30' AZ='CTR'> It is left unexplained what goals motivate conversational co-operation . </S>
9407011.xml:<S ID='S-31' AZ='CTR'> The problem with systems which impose co-operativity in the form of automatic goal adoption is that this makes it impossible to reason about cases in which one might want to violate these rules , especially in cases where the conversational co-operation might conflict with the agent 's personal goals . </S>
9407011.xml:<S ID='S-32' AZ='CTR'> We are developing an alternate approach that takes a step back from the strong plan-based approach . </S>
9407011.xml:<S ID='S-34' AZ='CTR'> While many of the intuitions underlying these approaches seems close to right , we claim it is a mistake to attempt to analyze this behavior as arising entirely from the agent 's high-level goals . </S>
9407011.xml:<S ID='S-62' AZ='CTR'> So we do not adopt the model suggested by <REF TYPE='A'>Shoham and Tennenholtz 1992</REF> in which agents ' behavior cannot violate the defined social laws . </S>
9407011.xml:<S ID='S-225' AZ='CTR'> We have also argued that an architecture that uses obligations provides a much simpler implementation than the strong plan-based approaches . </S>
9408004.xml:<S ID='S-9' AZ='CTR'> Little work has been done on the complexity of algorithms used to parse with a principle-based grammar , since such grammars do not exist as accepted mathematically well-defined constructs . </S>
9408004.xml:<S ID='S-10' AZ='CTR'> It has been estimated that in general , principle-based parsing can only be accomplished in exponential time , i.e. <EQN/> <REF  TYPE='P'>Berwick and Weinberg 1984</REF>, <REF  TYPE='P'>Weinberg 1988</REF> . </S>
9408004.xml:<S ID='S-13' AZ='CTR'> Although attempts have been made to modify PS grammars / parsers to cope with extragrammatical input <REF TYPE='P'>Carbonell and Hayes 1983</REF> , <REF TYPE='P'>Douglas and Dale 1992</REF> , <REF TYPE='P'>Jensen et al. 1983</REF> , <REF TYPE='P'>Mellish 1989</REF> , this is a feature which has to be ` added on ' and tends to affect the statement of the grammar . </S>
9408004.xml:<S ID='S-40' AZ='CTR'> The work on stochastic context-free grammars suggests a different set of results , in that the specific categories involved in expansions are all important . </S>
9408004.xml:<S ID='S-44' AZ='CTR'> Since it can be argued that the probabilistic information on lexical items will be needed independently , there is no need to use category specific information in assigning probabilities to syntactic configurations . </S>
9408006.xml:<S ID='S-6' AZ='CTR'> The chief modifications to the standard Prolog ` grammar rule ' format are of two types : one or more right-hand side ( RHS ) items may be marked as ` heads ' , and one or more RHS items may be marked as ` ignorable ' . </S>
9408006.xml:<S ID='S-36' AZ='CTR'> In the central role of heads , LHIP resembles parsers devised by <REF TYPE='A'>Kay 1989</REF> and <REF TYPE='A'>van Noord 1991</REF> ; in other respects , including the use which is made of heads , the approaches are rather different , however . </S>
9408006.xml:<S ID='S-162' AZ='CTR'> Experience so far suggests that systems like LHIP may in the right circumstances provide an alternative to these approaches . </S>
9408011.xml:<S ID='S-9' AZ='CTR'> His notion of similarity seems to agree with our intuitions in many cases , but it is not clear how it can be used directly to construct word classes and corresponding models of association . </S>
9408011.xml:<S ID='S-13' AZ='CTR'> Most other class-based modeling techniques for natural language rely instead on `` hard '' Boolean classes <REF TYPE='P'>Brown et al. 1990</REF> . </S>
9408011.xml:<S ID='S-14' AZ='CTR'> Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words , a potentially unreliable source of information as we noted above . </S>
9408011.xml:<S ID='S-41' AZ='CTR'> However , this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes . </S>
9408011.xml:<S ID='S-42' AZ='CTR'> It turns out that the problem is avoided by our clustering technique , since it does not need to compute the KL distance between individual word distributions , but only between a word distribution and average distributions , the current cluster centroids , which are guaranteed to be nonzero whenever the word distributions are . </S>
9408011.xml:<S ID='S-43' AZ='CTR'> This is a useful advantage of our method compared with agglomerative clustering techniques that need to compare individual objects being considered for grouping . </S>
9408014.xml:<S ID='S-25' AZ='CTR'> It relates to the inadequacy of the dominant theories in linguistics to capture ` shades ' of meaning or degrees of acceptability which are often recognized by people outside the field as important inherent properties of natural language . </S>
9408014.xml:<S ID='S-38' AZ='CTR'> However , this does not capture important structural properties of natural language . </S>
9408014.xml:<S ID='S-39' AZ='CTR'> Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences . </S>
9408014.xml:<S ID='S-92' AZ='CTR'> Although there are compilation techniques <REF TYPE='P'>Mellish 1988</REF> which allow selectional constraints stated in this fashion to be implemented efficiently , the scheme is problematic in other respects . </S>
9408014.xml:<S ID='S-93' AZ='CTR'> To start with , the assumption of a small set of senses for a word is at best awkward because it is difficult to arrive at an optimal granularity for sense distinctions . </S>
9408014.xml:<S ID='S-94' AZ='CTR'> Disambiguation with selectional restrictions expressed as meaning postulates is also problematic because it is virtually impossible to devise a set of postulates that will always filter all but one alternative . </S>
9408014.xml:<S ID='S-108' AZ='CTR'> Although such an approach was applied with some success in a limited-domain system translating logical forms into database queries <REF TYPE='P' SELF="YES">Rayner and Alshawi 1992</REF> , it is likely to be impractical for language translation with tens of thousands of sense predicates and related axioms . </S>
9409004.xml:<S ID='S-61' AZ='CTR'> These approaches seem inappropriate for tackling our needs , either because they detect only local co-occurrences <REF TYPE='P'>Church et al. 1991</REF> , <REF TYPE='P'>Church and Hanks 1990</REF> , or because they extract many spurious co-occurrence triples <REF  TYPE='P'>Basili et al. 1992</REF>, <REF  TYPE='P'>Church and Hanks 1990</REF> . </S>
9409004.xml:<S ID='S-63' AZ='CTR'> On the other hand , the fact that these approaches extract co-occurrences without reliability on being verb-complements violates accuracy requirements . </S>
9409004.xml:<S ID='S-72' AZ='CTR'> The advantages and drawbacks of both approaches are diverse . </S>
9409004.xml:<S ID='S-75' AZ='CTR'> On the other hand , while <REF TYPE='A'>Basili et al. 1992</REF> implies hand-coding all the relevant words with semantic tags , <REF TYPE='A'>Resnik 1992</REF> needs a broad semantic taxonomy . </S>
9409004.xml:<S ID='S-100' AZ='CTR'> <REF TYPE='A'>Resnik 1992</REF> performed a similar learning process , but while he was only looking for the preferred class of object nouns , we are interested in all the possible classes ( SRs ) . </S>
9409004.xml:<S ID='S-102' AZ='CTR'> However , if the function to maximize doesn't have a monotone behavior ( as it is the case of Assoc ) the best-first search doesn't guarantee global optimals , but only local ones . </S>
9410001.xml:<A-S ID='A-0' AZ='CTR'> Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences . </A-S>
9410001.xml:<S ID='S-7' AZ='CTR'> However , such remedies have their drawbacks . </S>
9410001.xml:<S ID='S-8' AZ='CTR'> Firstly , even when the context is extended , some important influences may still not be modeled . </S>
9410001.xml:<S ID='S-9' AZ='CTR'> For example , dependencies between words exist at separations greater than those allowed for by trigrams ( for which long-distance N-grams <REF TYPE='P'>Jelinek et al. 1991</REF> are a partial remedy ) , and associating scores with parsing table states may not model all the important correlations between grammar rules . </S>
9410001.xml:<S ID='S-10' AZ='CTR'> Secondly , extending the model may greatly increase the amount of training data required if sparseness problems are to be kept under control , and additional data may be unavailable or expensive to collect . </S>
9410001.xml:<S ID='S-11' AZ='CTR'> Thirdly , one cannot always know in advance of doing the work whether extending a model in a particular direction will , in practice , improve results . </S>
9410001.xml:<S ID='S-12' AZ='CTR'> If it turns out not to , considerable ingenuity and effort may have been wasted . </S>
9410001.xml:<S ID='S-28' AZ='CTR'> This work thus goes beyond that of <REFAUTHOR>Iyer et al.</REFAUTHOR> by focusing on the methodological importance of corpus clustering , rather than just its usefulness in improving overall system performance , and by exploring in detail the way its effectiveness varies along the dimensions of language model type , language model complexity , and number of clusters used . </S>
9410001.xml:<S ID='S-29' AZ='CTR'> It also differs from <REFAUTHOR>Iyer et al.</REFAUTHOR> 's work by clustering at the utterance rather than the paragraph level , and by using a training corpus of thousands , rather than millions , of sentences ; in many speech applications , available training data is likely to be quite limited , and may not always be chunked into paragraphs . </S>
9410001.xml:<S ID='S-32' AZ='CTR'> The approach described here is quite different . </S>
9410001.xml:<S ID='S-33' AZ='CTR'> Firstly , it involves clustering whole sentences , not words . </S>
9410001.xml:<S ID='S-47' AZ='CTR'> For some applications , though , there is no obvious extrinsic basis for dividing the training corpus into clusters . </S>
9410001.xml:<S ID='S-48' AZ='CTR'> The ARPA air travel information ( ATIS ) domain is an example . </S>
9410001.xml:<S ID='S-58' AZ='CTR'> Unfortunately , whatever the criterion selected , it is in general impractical to find the optimal clustering of the data ; instead , one of a variety of algorithms must be used to find a locally optimal solution . </S>
9410001.xml:<S ID='S-62' AZ='CTR'> However , it makes sense to choose as a similarity measure the quantity we would like the final clustering arrangement to minimize : the expected entropy ( or , equivalently , perplexity ) of sentences from the domain . </S>
9410001.xml:<S ID='S-108' AZ='CTR'> A number of backoff schemes of various degrees of sophistication , including that of <REF TYPE='A'>Katz 1987</REF> , were tried , but none produced any improvement in performance , and several actually worsened it . </S>
9410005.xml:<S ID='S-46' AZ='CTR'> The constraints proposed by <REF TYPE='A'>Grosz et al. 1986</REF> fail in certain examples like the following ( read with pronouns destressed ) : </S>
9410005.xml:<S ID='S-49' AZ='CTR'> However the constraints and rules from <REF TYPE='A'>Grosz et al. 1986</REF> would fail to make a choice here between the co-specification possibilities for the pronouns in <EQN/> . </S>
9410005.xml:<S ID='S-63' AZ='CTR'> However , structural parallelism is a consequence of our ordering the Cf list by grammatical function and the preference for continuing over retaining . </S>
9410005.xml:<S ID='S-64' AZ='CTR'> Furthermore , the constraints suggested in <REF TYPE='A'>Grosz et al. 1986</REF> succeed in many cases without invoking an independent structural parallelism constraint , due to the distinction between continuing and retaining , which <REFAUTHOR>Kameyama</REFAUTHOR> fails to consider . </S>
9410006.xml:<S ID='S-42' AZ='CTR'> However the novel Wheels has many examples of reported dialogue such as </S>
9410006.xml:<S ID='S-43' AZ='CTR'> One might wonder whether the subject is She or Mr. Vale . </S>
9410006.xml:<S ID='S-52' AZ='CTR'> This doesn't happen with the <REFAUTHOR>Hobbs</REFAUTHOR> algorithm because it proposes interpretations in a sequential manner , one at a time . </S>
9410008.xml:<S ID='S-44' AZ='CTR'> The problem with using automatically derived categories is that even if they are in a sense real , meaning that they are supported by data , they may be difficult to explain for the unenthusiastic layman if the aim is to use the technique in retrieval tools . </S>
9410009.xml:<S ID='S-14' AZ='CTR'> Our work differs from theirs in scope and also in the exploration of various other directions . </S>
9410009.xml:<S ID='S-15' AZ='CTR'> The use we make of lexical functions as interlingual representations , does not respect their original <REFAUTHOR>Mel'cuk</REFAUTHOR> -ian interpretation . </S>
9410012.xml:<A-S ID='A-9' AZ='CTR'> The conclusions are broadly in agreement with those of <REF TYPE='A'>Merialdo 1994</REF> , but give greater detail about the contributions of different parts of the model . </A-S>
9410012.xml:<S ID='S-27' AZ='CTR'> However , some careful examination of the experiment is needed . </S>
9410012.xml:<S ID='S-28' AZ='CTR'> In the first place , <REFAUTHOR>Cutting et al.</REFAUTHOR> do not compare the success rate in their work with that achieved from a hand-tagged training text with no re-estimation . </S>
9410012.xml:<S ID='S-29' AZ='CTR'> Secondly , it is unclear how much the initial biasing contributes the success rate . </S>
9410012.xml:<S ID='S-30' AZ='CTR'> If significant human intervention is needed to provide the biasing , then the advantages of automatic training become rather weaker , especially if such intervention is needed on each new text domain . </S>
9410012.xml:<S ID='S-31' AZ='CTR'> The kind of biasing <REFAUTHOR>Cutting et al.</REFAUTHOR> describe reflects linguistic insights combined with an understanding of the predictions a tagger could reasonably be expected to make and the ones it could not . </S>
9410012.xml:<S ID='S-45' AZ='CTR'> Consulting the Baum-Welch re-estimation formulae suggests that the approach described is more appropriate , and this is confirmed by slightly greater tagging accuracy . </S>
9410012.xml:<S ID='S-52' AZ='CTR'> A better measure is the proportion of ambiguous words which are given the correct tag , where by ambiguous we mean that more than one tag was hypothesised . </S>
9410022.xml:<S ID='S-9' AZ='CTR'> I argue that Hidden Markov Models are unsuited to the task and I demonstrate the importance of having a computational tool which allows phonologists to experiment with F <EQN/> scaling parameters . </S>
9410022.xml:<S ID='S-24' AZ='CTR'> Hidden Markov Models ( HMMs ) <REF TYPE='P'>Huang et al. 1990</REF> offer a powerful statistical approach to this problem , though it is unclear how they could be used to recognise the units of interest to phonologists . </S>
9410022.xml:<S ID='S-25' AZ='CTR'> HMMs do not encode timing information in a way that would allow them to output , say , one tone per syllable ( or vowel ) . </S>
9410022.xml:<S ID='S-26' AZ='CTR'> Moreover , the same section of a pitch contour may correspond to either H or L tones . </S>
9410022.xml:<S ID='S-27' AZ='CTR'> For example , a H between two Hs looks just like an L between two Ls . </S>
9410022.xml:<S ID='S-28' AZ='CTR'> There is no principled upper bound on the amount of context that needs to be inspected in order to resolve the ambiguity , leading to a multiplication of state information required by the HMM and problems for training it . </S>
9410022.xml:<S ID='S-53' AZ='CTR'> Therefore , phonologists who do not wish to limit themselves to the transcriptions which result from certain parameter settings in the phonetic interpretation function would be better off working directly with number sequences like the last row in <CREF/> . </S>
9410032.xml:<S ID='S-2' AZ='CTR'> Developed before the era of NL generation , the system EXPOUND of <REF TYPE='A'>Chester 1976</REF> can be characterized as an example of direct translation : Although a sophisticated linearization is applied on the input ND proofs , the steps are then translated locally in a template driven way . </S>
9410032.xml:<S ID='S-3' AZ='CTR'> ND proofs were tested as input to an early version of the MUMBLE system of <REF TYPE='A'>McDonald 1983</REF> , the main aim however , was to show the feasibility of the architecture . </S>
9410032.xml:<S ID='S-4' AZ='CTR'> A more recent attempt can be found in THINKER <REF TYPE='P'>Edgar and Pelletier 1993</REF> , which implements several interesting but isolated proof presentation strategies , without giving a comprehensive underlying model . </S>
9410032.xml:<S ID='S-78' AZ='CTR'> In contrast with operators employed in RST-based planners that split goals according to the rhetorical structures , our operators encode standard schemata for presenting proofs , which contain subgoals . </S>
9410032.xml:<S ID='S-172' AZ='CTR'> Although our overall presentation mechanism has much in common with that of RST-based text planners , the top-down planning operators contain mostly complex presentation schemata , like those in schema-based planning . </S>
9410033.xml:<S ID='S-35' AZ='CTR'> In the literature of non-incremental generation , the need for defaults is hardly ever taken into account . </S>
9410033.xml:<S ID='S-38' AZ='CTR'> Nevertheless , they do not in sufficient depth answer the question of how to guide the processes of default handling and repair within a generator . </S>
9411021.xml:<S ID='S-25' AZ='CTR'> However , in a concurrent model , this kind of strict order is a hindrance and contingent conversions are required . </S>
9411023.xml:<S ID='S-1' AZ='CTR'> However , since conventional word-frequency-based abstract generation systems <REF TYPE='P'>Kuhn 1958</REF> are lacking in inter-sentential or discourse-structural analysis , they are liable to generate incoherent abstracts . </S>
9411023.xml:<S ID='S-2' AZ='CTR'> On the other hand , conventional knowledge or script-based abstract generation systems <REF TYPE='P'>Lehnert 1980</REF> , <REF TYPE='P'>Fum 1986</REF> , owe their success to the limitation of the domain , and cannot be applied to document with varied subjects , such as popular scientific magazine . </S>
9411023.xml:<S ID='S-7' AZ='CTR'> These theories all depend on extra-linguistic knowledge , the accumulation of which presents a problem in the realization of a practical analyzer . </S>
9411023.xml:<S ID='S-8' AZ='CTR'> <REFAUTHOR>Cohen</REFAUTHOR> proposed a framework for analyzing the structure of argumentative discourse <REF TYPE='P'>Cohen 1987</REF> , yet did not provide a concrete identification procedure for ` evidence ' relationships between sentences , where no linguistic clues indicate the relationships . </S>
9411023.xml:<S ID='S-9' AZ='CTR'> Also , since only relationships between successive sentences were considered , the scope which the relationships cover cannot be analyzed , even if explicit connectives are detected . </S>
9411023.xml:<S ID='S-11' AZ='CTR'> However , no method for extracting the relationships from superficial linguistic expressions was described in their paper . </S>
9411023.xml:<S ID='S-101' AZ='CTR'> First , unlike conventional word-frequency-based abstract generation systems <REF TYPE='P'>Kuhn 1958</REF> , the generated abstract is consistent with the original text in that the connectives between sentences in the abstract reflect their relation in the original text . </S>
9411023.xml:<S ID='S-104' AZ='CTR'> Third , unlike conventional knowledge or script-based abstract generation systems <REF TYPE='P'>Lehnert 1980</REF> , <REF TYPE='P'>Fum 1986</REF> , the rhetorical structure extraction does not need prepared knowledge or scripts related to the original text , and can be used for texts of any domain , so long as they contain enough rhetorical expressions to be expository writings . </S>
9412005.xml:<A-S ID='A-2' DOCUMENTC='S-165' AZ='CTR'> Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source . </A-S>
9412005.xml:<S ID='S-2' AZ='CTR'> While evidence exists that infants are sensitive to these information sources , we know of no measurements of their usefulness . </S>
9412005.xml:<S ID='S-8' AZ='CTR'> However , the problem we examined is different -- we want to know how infants segment speech before knowing which phonemic sequences form words . </S>
9412005.xml:<S ID='S-14' AZ='CTR'> These studies demonstrated infants ' perceptive abilities without demonstrating the usefulness of infants ' perceptions . </S>
9412005.xml:<S ID='S-22' AZ='CTR'> However , <REF TYPE='A'>Fisher and Tokura in press</REF> found no evidence that prosody can accurately predict word boundaries , so the task of finding words remains . </S>
9412005.xml:<S ID='S-40' AZ='CTR'> However , this simplistic method is inefficient ; for instance , the length of lexical indices are arbitrary with respect to properties of the words themselves ( e.g. , in Hypothesis 2 , there is no reason why /jul/ was assigned the index ` 10 ' -- length two -- instead of ` 9 ' -- length one ) . </S>
9412008.xml:<S ID='S-18' AZ='CTR'> Therefore , the HMM model may suggest an improper character sequence as a word . </S>
9412008.xml:<S ID='S-19' AZ='CTR'> Furthermore , since nonterminal symbols of CFG are derived from a statistical analysis of word collocations , their number tends to be large and so the number of CFG rules are also large . </S>
9412008.xml:<S ID='S-20' AZ='CTR'> They assumed compound nouns consist of only one character words and two character words . </S>
9412008.xml:<S ID='S-21' AZ='CTR'> It is questionable whether this method can be extended to handle cases that include more than two character words without lowering accuracy . </S>
9502004.xml:<S ID='S-12' AZ='CTR'> Top-down algorithms are not well suited for processing grammatical theories like Categorial Grammar or HPSG that would only allow very general predictions because they make use of general schemata instead of construction-specific rules . </S>
9502004.xml:<S ID='S-32' AZ='CTR'> But , clearly it is not restrictive enough to find only the predicate name of the base case for a given goal . </S>
9502004.xml:<S ID='S-124' AZ='CTR'> However , it is not clear how well the directed methods are applicable to grammars which do not depend on concatenation and have no unique ` left corner ' which should be connected to the start symbol . </S>
9502005.xml:<S ID='S-6' AZ='CTR'> Though <REF TYPE='A' SELF="YES">Gerdemann 1991</REF> showed how to modify the restriction function to make top-down information available for the bottom-up completion step , Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination . </S>
9502005.xml:<S ID='S-8' AZ='CTR'> However , evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing , displaced or underspecified . </S>
9502005.xml:<S ID='S-9' AZ='CTR'> Furthermore , <REF TYPE='A'>Martinovic and Strzalkowski 1992</REF> and others have observed that a simple head-first reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules . </S>
9502005.xml:<S ID='S-13' AZ='CTR'> <REF TYPE='A' SELF="YES">Minnen et al. 1995</REF> observe that the EAA is computationally infeasible , because it demands the investigation of almost all possible permutations of a grammar . </S>
9502005.xml:<S ID='S-14' AZ='CTR'> Moreover , the interchanging of arguments in recursive procedures as proposed by <REFAUTHOR>Strzalkowski</REFAUTHOR> fails to guarantee that input and output grammars are semantically equivalent . </S>
9502005.xml:<S ID='S-24' AZ='CTR'> As <REF TYPE='A'>Shieber 1988</REF> noted , the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges . </S>
9502005.xml:<S ID='S-32' AZ='CTR'> By combining the off-line optimization process with a mixed bottom-up / top-down evaluation strategy , we can refrain from a complete reformulation of the grammar as , for example , in <REF SELF="YES" TYPE='A'>Minnen et al. 1995</REF> . </S>
9502005.xml:<S ID='S-49' AZ='CTR'> Empty or displaced heads present the principal goal-directedness problem for any head-driven generation approach <REF TYPE='P'>Shieber et al. 1990</REF> , <REF TYPE='A'>Koenig 1994b</REF> , <REF TYPE='A' SELF="YES">Gerdemann and Hinrichs in press</REF> , where empty head refers not just to a construction in which the head has an empty phonology , but to any construction in which the head is partially unspecified . </S>
9502005.xml:<S ID='S-124' AZ='CTR'> It is shown that combining off-line techniques with an advanced Earley-style generator provides an elegant solution to the general problem that empty or displaced heads pose for conventional head-driven generation . </S>
9502006.xml:<S ID='S-4' AZ='CTR'> However , there is often a trade-off between run-time efficiency and factors important for rapid and accurate system development , such as perspicuity of notation , ease of debugging , speed of compilation and the size of its output , and the independence of the morphological and lexical components . </S>
9502009.xml:<S ID='S-51' AZ='CTR'> Although the performance of the presented technique seems to be quite good , we think that some of the detected flaws could possibly be addressed . </S>
9502009.xml:<S ID='S-52' AZ='CTR'> Noise due to polysemy of the nouns involved seems to be the main obstacle for the practicality of the technique . </S>
9502009.xml:<S ID='S-53' AZ='CTR'> It makes the association score prefer incorrect classes and jump on over-generalizations . </S>
9502009.xml:<S ID='S-74' AZ='CTR'> This scheme seems to present two problematic features ( see <REF TYPE='A' SELF="YES">Ribas 1994a</REF> for more details ) . </S>
9502009.xml:<S ID='S-75' AZ='CTR'> First , it doesn't take dependency relationships introduced by hyperonymy into account . </S>
9502009.xml:<S ID='S-76' AZ='CTR'> Second , nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns . </S>
9502009.xml:<S ID='S-113' AZ='CTR'> However , we have tested SRs on a WSS task , using the following scheme . </S>
9502014.xml:<S ID='S-1' AZ='CTR'> But it suffers a number of drawbacks , especially when viewed from a computational perspective . </S>
9502014.xml:<S ID='S-2' AZ='CTR'> First , the precise order in which quantifiers are scoped and ellipses resolved determines the final interpretation of elliptical sentences . </S>
9502014.xml:<S ID='S-3' AZ='CTR'> It is hard to see how <REFAUTHOR>Dalrymple et al.</REFAUTHOR> 's analysis could be implemented within a system employing a pipelined architecture that , say , separates quantifier scoping out from other reference resolution operations -- this would seem to preclude the generation of some legitimate readings . </S>
9502014.xml:<S ID='S-4' AZ='CTR'> Yet many systems , for good practical reasons , employ this kind of architecture . </S>
9502014.xml:<S ID='S-5' AZ='CTR'> Second , without additional constraints , <REFAUTHOR>Dalrymple et al.</REFAUTHOR> slightly overgenerate readings for sentences like </S>
9502014.xml:<S ID='S-6' AZ='CTR'> <REF TYPE='A'>Kehler 1993a</REF> has convincingly argued that this problem arises because <REFAUTHOR>Dalrymple et al.</REFAUTHOR> do not distinguish between merely co-referential and co-indexed ( in his terminology , role-linked ) expressions . </S>
9502014.xml:<S ID='S-7' AZ='CTR'> Third , though perhaps less importantly , higher-order unification going beyond second-order matching is required for resolving ellipses involving quantification . </S>
9502014.xml:<S ID='S-8' AZ='CTR'> This increases the computational complexity of the ellipsis resolution task . </S>
9502014.xml:<S ID='S-14' AZ='CTR'> However , we extend the notion of strict and sloppy identity to deal with more than just pronouns . </S>
9502014.xml:<S ID='S-22' AZ='CTR'> So , for example , whether two terms with identical meanings are merely co-referential or are co-indexed is the kind of information that may get lost : the difference amounts to two ways of composing the same meaning . </S>
9502014.xml:<S ID='S-98' AZ='CTR'> Besides illustrating scope parallelism , this is an example where <REFAUTHOR>Dalrymple et al.</REFAUTHOR> have to resort to higher-order unification beyond second-order matching . </S>
9502014.xml:<S ID='S-151' AZ='CTR'> This leads to an uninterpretable cyclic QLF in much the same way that <REFAUTHOR>Dalrymple et al.</REFAUTHOR> obtain a violation of the occurs check on sound unification . </S>
9502014.xml:<S ID='S-159' AZ='CTR'> <REFAUTHOR>Dalrymple et al.</REFAUTHOR> block the reading by a more artificial restriction on the depth of embedding of expressions in logical forms ; they lack the means for distinguishing between coindexed and merely co-referential expressions . </S>
9502014.xml:<S ID='S-174' AZ='CTR'> Neither <REFAUTHOR>Kamp</REFAUTHOR> nor <REFAUTHOR>Kehler</REFAUTHOR> extend their copying / substitution mechanism to anything besides pronouns , as we have done . </S>
9502014.xml:<S ID='S-175' AZ='CTR'> In <REFAUTHOR>Kehler</REFAUTHOR> 's case , it is hard to see how his role assignment functions can be extended to deal with non-referential terms in the desired manner . </S>
9502014.xml:<S ID='S-213' AZ='CTR'> In addition , it cures a slight overgeneration problem in <REFAUTHOR>Dalrymple et al.</REFAUTHOR> 's account . </S>
9502015.xml:<A-S ID='A-0' DOCUMENTC='S-7' AZ='CTR'> We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures . </A-S>
9502015.xml:<A-S ID='A-2' DOCUMENTC='S-15' AZ='CTR'> The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . </A-S>
9502015.xml:<S ID='S-3' AZ='CTR'> However , there are cases where a single constituent appears to yield more than one contribution to the meaning of an utterance . </S>
9502015.xml:<S ID='S-4' AZ='CTR'> This is most obvious in , but is not limited to , sentences involving coordination . </S>
9502015.xml:<S ID='S-5' AZ='CTR'> In example <CREF/> , for instance , NAFTA is the object of two different verbs : </S>
9502015.xml:<S ID='S-6' AZ='CTR'> Since the hallmark of the linear logic approach is to ensure that f-structure contributions are utilized exactly once in a derivation , such constructions would at first glance appear to be problematic for the approach . </S>
9502015.xml:<S ID='S-25' AZ='CTR'> As noted by <REF TYPE='A'>Hudson 1976</REF> , however , not all examples of RNR involve coordinate structures : </S>
9502015.xml:<S ID='S-26' AZ='CTR'> Obviously , such cases fall outside of the purview of the coordination schema . </S>
9502015.xml:<S ID='S-29' AZ='CTR'> While this approach has some drawbacks , we do not offer a competing analysis of the syntax of sentences like <CREF/> here . </S>
9502015.xml:<S ID='S-42' AZ='CTR'> However , an unwelcome consequence of this approach , which appears to have gone unnoticed in the literature , arises in cases in which more than two verbs are conjoined . </S>
9502015.xml:<S ID='S-46' AZ='CTR'> This is the wrong result ; in a sentence such as ` Hillary wanted , found , and supported two candidates ' , the desired result is where one quantifier scopes over both extensional verbs ( that is , Hillary found and supported the same two candidates ) , just as in the case where all the verbs are extensional . </S>
9502015.xml:<S ID='S-47' AZ='CTR'> Further , there does not seem to be an obvious way to modify the <REFAUTHOR>Partee and Rooth</REFAUTHOR> proposal so as to produce the correct result , the problem being that the ability to copy quantifiers inherent in their schema is too unrestricted . </S>
9502015.xml:<S ID='S-48' AZ='CTR'> A second problem with the account is that , as with <REFAUTHOR>Steedman</REFAUTHOR> 's coordination schema , <REFAUTHOR>Partee and Rooth</REFAUTHOR> 's type-raising strategy only applies to coordinate structures . </S>
9502015.xml:<S ID='S-49' AZ='CTR'> However , the need to type-raise extends to cases not involving coordination , as in sentence <CREF/> . </S>
9502015.xml:<S ID='S-65' AZ='CTR'> Because linear logic does not have any form of logical contraction ( as is inherent in the approaches discussed earlier ) , cases where resources are shared appear to be problematic in this framework . </S>
9502015.xml:<S ID='S-66' AZ='CTR'> Intuitively , however , the need for the multiple use of an f-structure meaning results not from the appearance of a particular lexical item ( e.g. , a conjunction ) or a particular syntactic construction ( e.g. , parasitic gap constructions ) , but instead results from multiple paths to it from within the f-structure that contains it , where structure sharing is motivated on syntactic grounds . </S>
9502015.xml:<S ID='S-145' AZ='CTR'> While the reading generated by <REFAUTHOR>Partee and Rooth</REFAUTHOR> is derivable , it requires three quantifiers and thus uses QNP duplication twice , which is less preferred than the reading requiring two quantifiers which uses QNP duplication once . </S>
9502015.xml:<S ID='S-146' AZ='CTR'> Also , it allows some flexibility in cases where pragmatics strongly suggests that quantifiers are copied and distributed for multiple extensional verbs ; unlike the <REFAUTHOR>Partee and Rooth</REFAUTHOR> account , this would apply equally to the case where there are also intensional verbs and the case where there are not . </S>
9502015.xml:<S ID='S-150' AZ='CTR'> The resulting account does not suffer from overgeneration inherent in other approaches , and applies equally to cases of resource sharing that do not involve coordination . </S>
9502015.xml:<S ID='S-151' AZ='CTR'> Furthermore , it lends itself readily to an extension for the intensional verb case that has advantages over the widely-assumed account of <REF TYPE='A'>Partee and Rooth 1983</REF> . </S>
9502018.xml:<S ID='S-12' AZ='CTR'> In <CREF/> , the second sentence is an elaboration of the first , and they therefore refer to aspects of the same event rather than to two sequential events . </S>
9502018.xml:<S ID='S-15' AZ='CTR'> The problem for practical systems is twofold : we could assume that in the case of narrative the <REFAUTHOR>Kamp</REFAUTHOR> / <REFAUTHOR>Hinrichs</REFAUTHOR> / <REFAUTHOR>Partee</REFAUTHOR> algorithm is the default , but each time the default is applied we would need to check all our available world knowledge to see whether there isn't a world knowledge postulate which might be overriding this assumption . </S>
9502018.xml:<S ID='S-16' AZ='CTR'> Clearly this would make the processing of text a very expensive operation . </S>
9502018.xml:<S ID='S-83' AZ='CTR'> This approach improves upon earlier work on discourse structure such as <REF TYPE='A'>Lascarides and Asher 1991</REF> and <REF TYPE='A'>Kehler 1994a</REF> in reducing the number of possible ambiguities ; it is also more precise than the <REFAUTHOR>Kamp</REFAUTHOR> / <REFAUTHOR>Hinrichs</REFAUTHOR> / <REFAUTHOR>Partee</REFAUTHOR> approach in that it takes into account ways in which the apparent defaults can be overridden and differentiates between events and activities , which behave differently in narrative progression . </S>
9502021.xml:<S ID='S-7' AZ='CTR'> While these techniques can yield significant improvements in performance , the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable . </S>
9502021.xml:<S ID='S-8' AZ='CTR'> This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power . </S>
9502021.xml:<S ID='S-78' AZ='CTR'> LIG are more powerful than CFG and are known to be weakly equivalent to Tree Adjoining Grammar , Combinatory Categorial Grammar , and Head Grammar <REF SELF="YES" TYPE='P'>Vijay-Shanker and Weir 1994</REF> . </S>
9502022.xml:<S ID='S-9' AZ='CTR'> While we know of previous work which associates scores with feature structures <REF TYPE='P'>Kim 1994</REF> are not aware of any previous treatment which makes explicit the link to classical probability theory . </S>
9502022.xml:<S ID='S-25' AZ='CTR'> This has the arguably unwelcome consequence that PCFGs are unable to make certain discriminations between trees which differ only in their configuration . </S>
9502022.xml:<S ID='S-38' AZ='CTR'> Also , because the grammar has no means of enforcing number agreement , the system systematically prefers plurals to singulars , even when doing this will lead to agreement clashes . </S>
9502022.xml:<S ID='S-39' AZ='CTR'> Thus `` buses stop '' has estimated <EQN/> , `` bus stop '' and `` buses stops '' both have probability <EQN/> and `` bus stops '' has probability <EQN/> . </S>
9502022.xml:<S ID='S-40' AZ='CTR'> This behaviour is clearly unmotivated by the corpus , and arises purely because of the inadequacy of the probabilistic model . </S>
9502023.xml:<A-S ID='A-1' DOCUMENTC='S-5' AZ='CTR'> The analysis in <REF TYPE='A'>Partee 1984</REF> of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after . </A-S>
9502023.xml:<S ID='S-5' ABSTRACTC='A-1' AZ='CTR'> The analysis of sentences such as <CREF/> in <REF TYPE='A'>de Swart 1991</REF> , within the framework of Discourse Representation Theory ( DRT ) <REF TYPE='P'>Kamp 1981</REF> gives the wrong truth-conditions , when the temporal connective in the sentence is before or after . </S>
9502023.xml:<S ID='S-6' AZ='CTR'> In DRT , such sentences trigger box-splitting with the eventuality of the subordinate clause and an updated reference time in the antecedent box , and the eventuality of the main clause in the consequent box , causing undesirable universal quantification over the reference time . </S>
9502023.xml:<S ID='S-61' AZ='CTR'> As noted in <REF TYPE='A'>Partee 1984</REF> , this analysis does not extend in a straightforward manner to cases in which the operator when is replaced by ( an unrestricted ) before or after , in such quantified contexts . </S>
9502023.xml:<S ID='S-62' AZ='CTR'> Constructing a similar DRS for such sentences gives the wrong truth conditions . </S>
9502023.xml:<S ID='S-67' AZ='CTR'> Paraphrasing this , we could say that John lights up cigarettes at all times preceding each phone call , not just once preceding each phone call . </S>
9502023.xml:<S ID='S-73' AZ='CTR'> <REF TYPE='A'>de Swart 1991</REF> notes that simply moving <EQN/> to the right-hand box does not agree with <REFAUTHOR>Hinrichs</REFAUTHOR> 's assumption , that temporal clauses are processed before the main clause , since they update the reference time , with respect to which the main clause will be interpreted . </S>
9502023.xml:<S ID='S-77' AZ='CTR'> This will not be predicted by the unselective binding of quantifiers in DRT , which quantify over all the free variables in their scope , in this case women-cat pairs . </S>
9502023.xml:<S ID='S-86' AZ='CTR'> <REFAUTHOR>De Swart</REFAUTHOR> 's solution does overcome <REFAUTHOR>Partee</REFAUTHOR> 's quantification problem , although not within DRT . </S>
9502023.xml:<S ID='S-87' AZ='CTR'> As such , the existential quantification in <CREF/> has to be stipulated , whereas our analysis acquires this existential quantification ` for free ' . </S>
9502023.xml:<S ID='S-88' AZ='CTR'> Our analysis of <REFAUTHOR>Partee</REFAUTHOR> 's quantification problem uses a different notion of reference time than that used by the accounts in the exposition above . </S>
9502024.xml:<S ID='S-11' AZ='CTR'> Whereas they can provide even better solutions intrinsically , they are usually ad-hoc and are lack of extensibility . </S>
9502024.xml:<S ID='S-16' AZ='CTR'> However , his experiment was not based on the errors in running texts but on artificial ones which were randomly generated by human . </S>
9502024.xml:<S ID='S-17' AZ='CTR'> Moreover , only one word error was considered though several word errors can occur simultaneously in the running text . </S>
9502024.xml:<S ID='S-22' AZ='CTR'> At the cost of the complete robustness , however , this algorithm degrades the efficiency of parsing , and generates many intermediate edges . </S>
9502024.xml:<S ID='S-61' AZ='CTR'> But this algorithm can handle only the errors of terminal symbols because it doesn't consider the errors of nonterminal nodes . </S>
9502024.xml:<S ID='S-62' AZ='CTR'> In the real text , however , the insertion , deletion , or inversion of a phrase - namely , nonterminal node - occurs more frequently . </S>
9502031.xml:<S ID='S-6' AZ='CTR'> However treating the sentence as a basic unit loses meaning when the ` sentence ' is incomplete or illformed . </S>
9502033.xml:<A-S ID='A-1' DOCUMENTC='S-1' AZ='CTR'> Several methods have been proposed to deal with each phenomenon separately , however none of proposed systems has considered the way of dealing both phenomena . </A-S>
9502033.xml:<S ID='S-1' ABSTRACTC='A-1' AZ='CTR'> However none of these methods has considered the way of dealing both phenomena in the same concrete system . </S>
9502035.xml:<A-S ID='A-1' DOCUMENTC='S-23' AZ='CTR'> In <REFAUTHOR>Gorrell</REFAUTHOR> 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add . </A-S>
9502035.xml:<S ID='S-22' AZ='CTR'> Although <REFAUTHOR>Gorrell</REFAUTHOR> proposes a general principle to guide initial attachment decisions ( Simplicity : No vacuous structure building ) , and specifies the conditions under which `` unconscious reanalysis '' may occur , the model leaves unspecified the problem of how the system may be implemented . </S>
9502035.xml:<S ID='S-122' AZ='CTR'> The current implementation shows that the success of an abstract model such as <REFAUTHOR>Gorrell</REFAUTHOR> 's depends crucially on the computational details of the processing algorithm used . </S>
9502037.xml:<S ID='S-8' AZ='CTR'> There is no lack of competing competence grammars available , but also no reason to expect that such grammars should be suited to a DOP approach , designed as they were to characterize the nature of linguistic competence rather than performance . </S>
9502038.xml:<A-S ID='A-1' AZ='CTR'> This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora . </A-S>
9502038.xml:<S ID='S-50' AZ='CTR'> However , the performance of the resulting HMM is very poor if no initial biases are used to help the training process find suitable parameters . </S>
9502038.xml:<S ID='S-53' AZ='CTR'> Choosing initial biases to help train a model is a subtle task in that it not only requires sound knowledge of the tag set used and the target language the model is aiming at , but it also requires a `` feel '' for how the initial biases may be modified during a given number of training iterations . </S>
9502038.xml:<S ID='S-54' AZ='CTR'> It is also sometimes frustrating that the linguistic knowledge used to create the initial biases gets `` optimized '' or `` trained '' away in subsequent iterations of training . </S>
9502038.xml:<S ID='S-67' AZ='CTR'> These results , however , are not directly comparable to the implementations described in this paper , since the tag sets employed differ to some extent . </S>
9502039.xml:<S ID='S-6' AZ='CTR'> While some text categorization systems give very good results , the major problem is that their quality is entirely based on the training set . </S>
9502039.xml:<S ID='S-7' AZ='CTR'> Profiles require a lot of data to converge and building a large representative training set is a real problem . </S>
9502039.xml:<S ID='S-8' AZ='CTR'> Moreover , this method assume that texts are monolingual and results will be affected when dealing with multilingual texts . </S>
9502039.xml:<S ID='S-9' AZ='CTR'> It does not care about natural language properties : it only considers texts as streams of characters . </S>
9502039.xml:<S ID='S-10' AZ='CTR'> There is no linguistic justification . </S>
9502039.xml:<S ID='S-55' AZ='CTR'> Theoritical issues in this field are in progress <REF TYPE='P'>Lucas et al. 1993</REF> , <REF TYPE='P'>Lucas 1992</REF> but as far as we know no implementation has been done yet . </S>
9503002.xml:<A-S ID='A-2' AZ='CTR'> This correlates closely with the much more laborious technique of determining and counting isoglosses , and is more accurate than the more familiar metric of computing Hamming distance based on whether vocabulary entries match . </A-S>
9503002.xml:<S ID='S-4' AZ='CTR'> But isoglosses are frustrating . </S>
9503002.xml:<S ID='S-5' AZ='CTR'> The first problem , as <REFAUTHOR>Gaston Paris</REFAUTHOR> noted  <REF TYPE='P'>Durand 1889</REF> , is that isoglosses rarely coincide . </S>
9503002.xml:<S ID='S-6' AZ='CTR'> At best , isoglosses for different features approach each other , forming vague bundles ; at worst , isoglosses may cut across each other , describing completely contradictory binary divisions of the dialect area . </S>
9503002.xml:<S ID='S-8' AZ='CTR'> Traditional dialectological methodology gives little guidance as to how to perform such reduction to one dimension . </S>
9503002.xml:<S ID='S-9' AZ='CTR'> A second problem is that many isoglosses do not neatly bisect the language area . </S>
9503002.xml:<S ID='S-10' AZ='CTR'> Often variants do not neatly line up on two sides of a line , but are intermixed haphazardly . </S>
9503002.xml:<S ID='S-11' AZ='CTR'> More importantly , for some sites information may be lacking , or the question is simply not applicable . </S>
9503002.xml:<S ID='S-12' AZ='CTR'> When comparing how various sites pronounce the first consonant of a particular word , it is meaningless to ask that question if the site does not use that word . </S>
9503002.xml:<S ID='S-13' AZ='CTR'> So the isogloss is incomplete and cannot be meaningfully compared with isoglosses based on different sets of sites . </S>
9503002.xml:<S ID='S-14' AZ='CTR'> The third problem is that most languages have dialect continua , such that the speech of one community differs little from the speech of its neighbours . </S>
9503002.xml:<S ID='S-15' AZ='CTR'> Even though the cumulative effects of such differences may be great when one considers the ends of the continua ( such as southern Italian versus northern French ) , still it seems arbitrary to draw major dialect boundaries between two villages with very similar speech patterns . </S>
9503002.xml:<S ID='S-16' AZ='CTR'> Such conundrums led <REFAUTHOR>Paris</REFAUTHOR> and others to conclude that the dialect boundary , and therefore the very notion of dialect , is an ill-defined concept . </S>
9503002.xml:<S ID='S-19' AZ='CTR'> There is however no firm agreement on just how to compute the distance matrices . </S>
9503002.xml:<S ID='S-27' AZ='CTR'> The need to figure out such systems as the comparative phonology of various linguistic sites can be very time-consuming and fraught with arbitrary choices . </S>
9503002.xml:<S ID='S-76' AZ='CTR'> This baseline was compared to several other approaches . </S>
9503002.xml:<S ID='S-91' AZ='CTR'> It would seem to be more accurate to assign a greater distance to substitutions involving greater phonetic distinctions . </S>
9503002.xml:<S ID='S-92' AZ='CTR'> Unfortunately I know of no comprehensive study on the differences between phones , at least not for all 277 contrasts made by <REFAUTHOR>Wagner</REFAUTHOR> . </S>
9503002.xml:<S ID='S-102' AZ='CTR'> All of these distance matrices were compared with the isogloss matrix , to see which of them gives results closest to that base method . </S>
9503002.xml:<S ID='S-117' AZ='CTR'> I compared agglomeration to a top-down method that <REF TYPE='A'>Kaufman and Rousseeuw 1990</REF> call partitioning around medoids . </S>
9503002.xml:<S ID='S-137' AZ='CTR'> Rather , it appears that the assumption behind this partitioning heuristic , that a site will be closer to the medoid of its own group than to the medoid of the other group , often fails to hold true in dialectology . </S>
9503002.xml:<S ID='S-138' AZ='CTR'> The lack of clean breaks between dialects and the fact that dialects of the same language may vary greatly in diameter ( i.e. , maximal intragroup distances ) both mean that the assumption will often be invalid . </S>
9503002.xml:<S ID='S-159' AZ='CTR'> This turns out to be quite a bit more precise than the common technique of measuring distances by judging etymon identity , and requires even less editorial work . </S>
9503002.xml:<S ID='S-161' ABSTRACTC='A-3' AZ='CTR'> As for clustering the sites into dialect areas , the familiar bottom-up agglomeration method proves superior to top-down partitioning around medoids . </S>
9503004.xml:<S ID='S-12' AZ='CTR'> For a statistical tagger however , a big tagset may be a major problem . </S>
9503005.xml:<S ID='S-3' AZ='CTR'> However standard formalisations of LFG do not capture its strikingly simple underlying intuitions . </S>
9503005.xml:<S ID='S-5' AZ='CTR'> The main point of the present paper is to show that such detours are unnecessary . </S>
9503009.xml:<A-S ID='A-1' AZ='CTR'> Unlike previous work , the algorithm categorizes word tokens in context instead of word types . </A-S>
9503009.xml:<S ID='S-8' AZ='CTR'> They require a relatively large tagged training text . </S>
9503009.xml:<S ID='S-9' AZ='CTR'> Transformation-based tagging as introduced by <REF TYPE='A'>Brill 1993</REF> also requires a hand-tagged text for training . </S>
9503009.xml:<S ID='S-11' AZ='CTR'> Still , a lexicon is needed that specifies the possible parts of speech for every word . </S>
9503009.xml:<S ID='S-20' AZ='CTR'> What these approaches have in common is that they classify words instead of individual occurrences . </S>
9503009.xml:<S ID='S-21' AZ='CTR'> Given the widespread part-of-speech ambiguity of words this is problematic . </S>
9503009.xml:<S ID='S-22' AZ='CTR'> How should a word like `` plant '' be categorized if it has uses both as a verb and as a noun ? </S>
9503009.xml:<S ID='S-23' AZ='CTR'> How can a categorization be considered meaningful if the infinitive marker `` to '' is not distinguished from the homophonous preposition ? </S>
9503009.xml:<S ID='S-25' AZ='CTR'> This scheme fails for cases like `` The soldiers rarely come home '' vs. `` The soldiers will come home '' where the context is identical and information about the lexical item in question ( `` rarely '' vs. `` will '' ) is needed in combination with context for correct classification . </S>
9503009.xml:<S ID='S-112' AZ='CTR'> This differs from previous approaches <REF TYPE='P'>Finch and Chater 1992</REF> , <REF TYPE='P' SELF="YES">Schuetze 1993</REF> in which left and right context vectors of a word are always used in one concatenated vector . </S>
9503009.xml:<S ID='S-179' ABSTRACTC='A-2' AZ='CTR'> The main innovation is that the algorithm is able to deal with part-of-speech ambiguity , a pervasive phenomenon in natural language that was unaccounted for in previous work on learning categories from corpora . </S>
9503013.xml:<A-S ID='A-2' AZ='CTR'> However , possible computational applications have received less attention . </A-S>
9503013.xml:<S ID='S-6' AZ='CTR'> However , on-line semantic filtering for sentence processing does have drawbacks . </S>
9503013.xml:<S ID='S-7' AZ='CTR'> Firstly , for sentence processing using a serial architecture ( rather than one in which syntactic and semantic processing is performed in parallel ) , the savings in computation obtained from on-line filtering have to be balanced against the additional costs of performing semantic computations for parses of fragments which would eventually be ruled out anyway from purely syntactic considerations . </S>
9503013.xml:<S ID='S-9' AZ='CTR'> Secondly , the task of judging plausibility or anomaly according to context and real world knowledge is a difficult problem , except in some very limited domains . </S>
9503013.xml:<S ID='S-12' AZ='CTR'> Cases where statistical techniques seem less appropriate are where plausibility is affected by local context . </S>
9503013.xml:<S ID='S-13' AZ='CTR'> For example , consider the ambiguous sentence , </S>
9503013.xml:<S ID='S-14' AZ='CTR'> in the two contexts </S>
9503013.xml:<S ID='S-15' AZ='CTR'> Such cases involve reasoning with an interpretation in its immediate context , as opposed to purely judging the likelihood of a particular linguistic expression in a given application domain ( see e.g. <REF TYPE='A' SELF="YES">Cooper 1993</REF> for discussion ) . </S>
9503013.xml:<S ID='S-55' AZ='CTR'> Both Categorial approaches to incremental interpretation and approaches which use partial syntax trees get into difficulty in cases of left recursion . </S>
9503013.xml:<S ID='S-56' AZ='CTR'> Consider the sentence fragment , Mary thinks John . </S>
9503013.xml:<S ID='S-57' AZ='CTR'> A possible partial syntax tree is provided by Fig. <CREF/> . </S>
9503013.xml:<S ID='S-58' AZ='CTR'> However , this is not the only possible partial tree . </S>
9503013.xml:<S ID='S-59' AZ='CTR'> In fact there are infinitely many different trees possible . </S>
9503013.xml:<S ID='S-60' AZ='CTR'> The completed sentence may have an arbitrarily large number of intermediate nodes between the lower s node and the lower np . </S>
9503013.xml:<S ID='S-61' AZ='CTR'> For example , John could be embedded within a gerund e.g. Mary thinks John leaving here was a mistake , and this in turn could be embedded e.g. Mary thinks John leaving here being a mistake is surprising . </S>
9503013.xml:<S ID='S-62' AZ='CTR'> John could also be embedded within a sentence which has a sentence modifier requiring its own s node e.g. Mary thinks John will go home probably , and this can be further embedded e.g. Mary thinks John will go home probably because he is tired . </S>
9503013.xml:<S ID='S-63' AZ='CTR'> The problem of there being an arbitrary number of different partial trees for a particular fragment is reflected in most current approaches to incremental interpretation being either incomplete , or not fully word by word . </S>
9503013.xml:<S ID='S-116' AZ='CTR'> Most psycholinguistic experimentation has been concerned with which scope preferences are made , rather than the point at which the preferences are established <REF TYPE='P'>Kurtzman and MacDonald 1993</REF> . </S>
9503013.xml:<S ID='S-166' AZ='CTR'> This incorrectly rules out the possibility of the noun phrase referring to a rabbit which is in nothing at all . </S>
9503014.xml:<A-S ID='A-1' DOCUMENTC='S-188' AZ='CTR'> This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing . </A-S>
9503014.xml:<S ID='S-3' AZ='CTR'> However , by considering grammaticality judgements alone , there seems little justification for such a division . </S>
9503014.xml:<S ID='S-27' AZ='CTR'> For example , <REF TYPE='A'>Lakoff and Peters 1969</REF> argued that deletion accounts are inappropriate for certain constituent coordinations such as : </S>
9503014.xml:<S ID='S-28' AZ='CTR'> since the ` antecedent ' sentence John are alike and Mary are alike is nonsensical ( it is also ungrammatical if we consider number agreement ) . </S>
9503014.xml:<S ID='S-35' AZ='CTR'> Thus , although <REFAUTHOR>Lakoff and Peters</REFAUTHOR> 's arguments count against standard deletion analyses , they do not count as general arguments against a unified treatment of constituent and non-constituent coordination . </S>
9503014.xml:<S ID='S-48' AZ='CTR'> Problems with the latter requirement were pointed out by <REF TYPE='A'>Sag et al. 1985</REF> , who gave the following counterexamples : </S>
9503014.xml:<S ID='S-51' AZ='CTR'> As it stands , the account does not deal with examples such as the following , </S>
9503014.xml:<S ID='S-52' AZ='CTR'> Here the adverbial phrase would presumably be +MANNER , and the prepositional phrase , +TEMP . </S>
9503014.xml:<S ID='S-53' AZ='CTR'> Further examples which are problematic for <REFAUTHOR>Sag et al.</REFAUTHOR> are given by <REF TYPE='A'>Jorgensen and Abeille 1992</REF> . </S>
9503014.xml:<S ID='S-68' AZ='CTR'> Unfortunately , the ability to perform abstraction of categories with functional types ( which is required for <CREF/> ) also allows shared material to get different syntactic analyses , resulting in acceptance of all the sentences predicted by deletion accounts where identity of lexical categories and lexical semantics is respected , but not identity of syntactic structure . </S>
9503014.xml:<S ID='S-75' AZ='CTR'> This account makes reasonably good empirical predictions , though it does fail for the following examples : </S>
9503014.xml:<S ID='S-76' AZ='CTR'> In <CREF/> , each conjunct contains different numbers of modifiers of different types ( an adverbial phrase with two prepositional phrases ) . </S>
9503014.xml:<S ID='S-77' AZ='CTR'> In <CREF/> the subcategorisation order is swapped in the two conjuncts . </S>
9503014.xml:<S ID='S-91' AZ='CTR'> <REFAUTHOR>Goodall</REFAUTHOR> 's account does not deal with examples such as <CREF/> , which he argues to be examples of a different phenomenon . </S>
9503014.xml:<S ID='S-93' AZ='CTR'> There are various technical difficulties with <REFAUTHOR>Goodall</REFAUTHOR> 's account <REF TYPE='P'>van Oirsouw 1987</REF> , <REF TYPE='P'>Moltmann 1992</REF> . </S>
9503014.xml:<S ID='S-94' AZ='CTR'> There is also a fundamental problem concerning semantic interpretation of coordinated structures ( see <REF TYPE='A'>Moltmann 1992</REF> which provides a revised and more complex 3-D account based on <REF TYPE='A'>Muadz 1991</REF> ) . </S>
9503014.xml:<S ID='S-96' AZ='CTR'> However there is still a problem in dealing with examples where there are different numbers of modifiers , such as <CREF/> or the following : </S>
9503014.xml:<S ID='S-97' AZ='CTR'> The syntactic structure appropriate for TNT deliver efficiently has one S node and two VP nodes . </S>
9503014.xml:<S ID='S-98' AZ='CTR'> However , the structure for TNT deliver after 5 pm in Edinburgh requires one S node and three VP nodes ( or three S nodes and one VP node ) . </S>
9503014.xml:<S ID='S-99' AZ='CTR'> The two structures therefore fail to merge since the structure dominating the shared material TNT deliver must be identical . </S>
9503014.xml:<S ID='S-100' AZ='CTR'> The use of ordered phrase structure trees also excludes examples such as <CREF/> . </S>
9503014.xml:<S ID='S-102' AZ='CTR'> However , the way of characterising syntactic structure using ( parts of ) standard phrase structure trees results in an overly strict requirement of parallelism between the conjuncts . </S>
9503014.xml:<S ID='S-117' AZ='CTR'> As noted by <REFAUTHOR>Dahl and McCord</REFAUTHOR> , this mechanism means that SYSCONJ inherits the problems of nonsensical semantics which plague the deletion accounts , since John and Mary are alike is treated the same as John are alike and Mary are alike . </S>
9503014.xml:<S ID='S-118' AZ='CTR'> The mechanism also causes problems for dealing with nested coordination . </S>
9503014.xml:<S ID='S-119' AZ='CTR'> Consider the sentence : </S>
9503014.xml:<S ID='S-120' AZ='CTR'> The smallest constituent containing to study medicine when he was eleven is the verb phase wanted to study medicine when he was eleven . </S>
9503014.xml:<S ID='S-121' AZ='CTR'> However , if coordination of the first two conjuncts occurs at this level , it is difficult to see how to deal with the final conjunct . </S>
9503014.xml:<S ID='S-123' AZ='CTR'> Thus once something is popped off the stack its internal structure cannot be accessed by the coordination routine . </S>
9503014.xml:<S ID='S-124' AZ='CTR'> This rules out examples such as the following , </S>
9503014.xml:<S ID='S-125' AZ='CTR'> where the NP , some books is completed prior to the conjunction being reached . </S>
9503014.xml:<S ID='S-126' AZ='CTR'> Although processing accounts can provide reasonable coverage of the coordination data , the exact predictions often require detailed examination of the code . </S>
9503014.xml:<S ID='S-188' ABSTRACTC='A-1' AZ='CTR'> This paper has sketched various problems with some of the linguistic accounts of coordination . </S>
9503015.xml:<S ID='S-10' AZ='CTR'> Neither approach is without problems . </S>
9503015.xml:<S ID='S-11' AZ='CTR'> If a grammar is augmented with operations which are powerful enough to make most initial fragments constituents , then there may be unwanted interactions with the rest of the grammar ( examples of this in the case of CCG and the Lambek Calculus are given in Section <CREF/> ) . </S>
9503015.xml:<S ID='S-12' AZ='CTR'> The addition of extra operations also means that , for any given reading of a sentence there will generally be many different possible derivations ( so-called ` spurious ' ambiguity ) , making simple parsing strategies such as shift-reduce highly inefficient . </S>
9503015.xml:<S ID='S-13' AZ='CTR'> The limitations of the parsing approaches become evident when we consider grammars with left recursion . </S>
9503015.xml:<S ID='S-14' AZ='CTR'> In such cases a simple top-down parser will be incomplete , and a left corner parser will resort to buffering the input ( so won't be fully word-by-word ) . </S>
9503015.xml:<S ID='S-32' AZ='CTR'> The main difference between our approach and that of <REF TYPE='A' SELF="YES">Milward 1992</REF> , <REF TYPE='A' SELF="YES">Milward 1994a</REF> is that it is based on a more expressive grammar formalism , Applicative Categorial Grammar , as opposed to Lexicalised Dependency Grammar . </S>
9503015.xml:<S ID='S-35' AZ='CTR'> However , there is a corresponding problem of far greater non-determinism , with even unambiguous words allowing many possible transitions . </S>
9503015.xml:<S ID='S-50' AZ='CTR'> An unfortunate aspect of <REFAUTHOR>Bar-Hillel</REFAUTHOR> 's first system was that the application rule only ever resulted in a primitive type . </S>
9503015.xml:<S ID='S-51' AZ='CTR'> Hence , arguments with functional types had to correspond to single lexical items : there was no way to form the type np <EQN/> s for a non-lexical verb phrase such as likes Mary . </S>
9503015.xml:<S ID='S-62' AZ='CTR'> However , there are problems with having just composition , the most basic of the non-applicative operations . </S>
9503015.xml:<S ID='S-66' AZ='CTR'> Thus , the noun very old dilapidated car can get the unacceptable bracketing , [ [ very [ old dilapidated ] ] car ] . </S>
9503015.xml:<S ID='S-74' AZ='CTR'> Given that our arguments have produced a categorial grammar which looks very similar to HPSG , why not use HPSG rather than Applicative CG ? </S>
9503015.xml:<S ID='S-75' AZ='CTR'> The main reason is that Applicative CG is a much simpler formalism , which can be given a very simple syntax semantics interface , with function application in syntax mapping to function application in semantics <EQN/> . </S>
9503015.xml:<S ID='S-79' AZ='CTR'> However , unlike <REFAUTHOR>Bar-Hillel</REFAUTHOR> , we allow one argument to be absorbed at a time . </S>
9503017.xml:<S ID='S-31' AZ='CTR'> I claim that these assumptions must be dropped in order to explain the function of IRU 's in dialogue . </S>
9503017.xml:<S ID='S-56' AZ='CTR'> This representation of mutual belief differs from the common representation in terms of an iterated conjunction <REF TYPE='P'>Litman and Allen 1990</REF> in that : </S>
9503017.xml:<S ID='S-57' TYPE='ITEM' AZ='CTR' > it relocates information from mental states to the environment in which utterances occur ; </S>
9503017.xml:<S ID='S-58' TYPE='ITEM' AZ='CTR' > it allows one to represent the different kinds of evidence for mutual belief ; </S>
9503017.xml:<S ID='S-59' TYPE='ITEM' AZ='CTR' > it controls reasoning when discrepancies in mutual beliefs are discovered since evidence and assumptions can be inspected ; </S>
9503017.xml:<S ID='S-60' TYPE='ITEM' AZ='CTR' > it does not consist of an infinite list of statements . </S>
9503017.xml:<S ID='S-65' AZ='CTR'> This distinguishes this account from others that assume either that utterance actions always succeed or that they succeed unless the addressee previously believed otherwise <REF  TYPE='P'>Litman and Allen 1990</REF>, <REF  TYPE='P'>Grosz and Sidner 1990</REF> . </S>
9503017.xml:<S ID='S-110' AZ='CTR'> This assumption is challenged by a number of cases in naturally occurring dialogues where inferences that follow from what has been said are made explicit . </S>
9503017.xml:<S ID='S-126' AZ='CTR'> A subcase of ensuring that certain inferences get made involves the juxtaposition of two propositions . </S>
9503017.xml:<S ID='S-127' AZ='CTR'> These cases challenge the assumption that : <CREF/> The context of a discourse is an undifferentiated set of propositions with no specific relations between them . </S>
9503017.xml:<S ID='S-199' AZ='CTR'> This means that these accounts cannot represent the fact that a belief must be supported by some kind of evidence and that the evidence may be stronger or weaker . </S>
9503017.xml:<S ID='S-200' AZ='CTR'> It also follows from <CREF/> that these models assume that agents are not autonomous , or at least do not have control over their own mental states . </S>
9503017.xml:<S ID='S-201' AZ='CTR'> But belief revision is surely an autonomous process ; agents can choose whether to accept a new belief or revise old beliefs <REF  TYPE='P'>Galliers 1991</REF>, <REF  TYPE='P'>Grosz and Sidner 1990</REF> . </S>
9503018.xml:<S ID='S-9' AZ='CTR'> But these modes are simply the extremes of possibility and to my knowledge , no previous work has proposed any principles for when to include optional information , or any way of testing the proposed principles to see how they are affected by the conversants and their processing abilities , by the task , by the communication channel , or by the domain . </S>
9503018.xml:<S ID='S-40' AZ='CTR'> To my knowledge , all previous work on dialogue has simply assumed that an agent should never tell an agent facts that the other agent already knows . </S>
9503018.xml:<S ID='S-41' AZ='CTR'> The hypotheses in figure <CREF/> seem completely plausible , but the relationship of cognitive effort to dialogue behavior has never been explored . </S>
9503018.xml:<S ID='S-167' AZ='CTR'> The use of the method and the focus of this work are novel : previous work has focused on determining underlying mechanisms for cooperative strategies rather than on investigating when a strategy is effective . </S>
9503018.xml:<S ID='S-168' AZ='CTR'> To my knowledge , no previous work on dialogue has ever argued that conversational agents ' resource limits are a major factor in determining effective conversational strategies in collaboration . </S>
9503018.xml:<S ID='S-172' AZ='CTR'> Here we compared two discourse strategies : All-Implicit and Explicit-Warrant . </S>
9503023.xml:<S ID='S-1' AZ='CTR'> This approach bears comparison with probabilistic systems , but has the advantage that negative as well as positive information can be modelled . </S>
9503023.xml:<S ID='S-65' AZ='CTR'> If embedded syntactic constituents are sought in a single pass , this can lead to computation
9503023.xml:<S ID='S-186' AZ='CTR'> However , this approach does not fully capture the sense in which inhibitory factors play a negative and not just a neutral role . </S>
9503023.xml:<S ID='S-195' AZ='CTR'> However , with positive data alone a problem of over generalization arises : the postulated grammar may be a superset of the real grammar , and sentences that are outside the real grammar could be accepted . </S>
9503023.xml:<S ID='S-211' AZ='CTR'> The traditional problems of training times do not arise . </S>
9503025.xml:<S ID='S-2' AZ='CTR'> However , they were derived manually through psychological experiments . </S>
9503025.xml:<S ID='S-5' AZ='CTR'> However , using the co-occurrence statistics requires a huge corpus that covers even most rare words . </S>
9503025.xml:<S ID='S-35' AZ='CTR'> However , this simple definition seems unnatural because it does not reflect word frequency . </S>
9503025.xml:<S ID='S-107' ABSTRACTC='A-1' AZ='CTR'> For the word sense disambiguation based on the context similarity , co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was advantageous over distance vectors from the Collins English Dictionary ( head words + definition words ) . </S>
9503025.xml:<S ID='S-108' AZ='CTR'> For learning or meanings from example words , distance vectors gave remarkably higher precision than co-occurrence vectors . </S>
9504002.xml:<S ID='S-28' AZ='CTR'> HMM taggers often perform poorly on unknown words . </S>
9504002.xml:<S ID='S-125' AZ='CTR'> This seems to go against the `` folklore '' of the tagging community , where smaller tagsets are often held to be better for obtaining good accuracy . </S>
9504006.xml:<S ID='S-13' AZ='CTR'> The problem with this approach is that the cue may only be an infrequent indicator of a particular type of shift . </S>
9504006.xml:<S ID='S-14' AZ='CTR'> If we want to construct a general theory of discourse than we want to know about the whole range of cues serving this function . </S>
9504006.xml:<S ID='S-17' AZ='CTR'> A second problem with previous research is that the criteria for identifying discourse structure are not always made explicit . </S>
9504006.xml:<S ID='S-141' AZ='CTR'> This result challenges the claims of recent discourse theories <REF TYPE='P'>Grosz and Sidner 1986</REF>, <REF TYPE='P'>Reichman 1985</REF> which argue for a the close relation between cue words and discourse structure . </S>
9504006.xml:<S ID='S-144' AZ='CTR'> Another focus of current research has been the modelling of speaker and listener goals <REF TYPE='P'>Allen and Perrault 1980</REF> , <REF TYPE='P'>Cohen and Levesque 1985</REF> but there has been little research on real dialogues investigating how goals are communicated and inferred . </S>
9504006.xml:<S ID='S-148' AZ='CTR'> In addition our methodology is different from other studies because we have attempted to provide an explanation for whole dialogues rather than fragments of dialogues , and used explicit criteria in a bottom-up manner to identify discourse structures . </S>
9504007.xml:<S ID='S-9' AZ='CTR'> Previous studies of the discourse structure of multi-participant dialogues have often factored out the role of MIXED-INITIATIVE , by allocating control to one participant <REF TYPE='P'>Grosz 1977</REF> , <REF TYPE='P'>Cohen 1984</REF> , or by assuming a passive listener <REF  TYPE='P'>McKeown 1985</REF>, <REF  TYPE='P'>Cohen 1987</REF> . </S>
9504007.xml:<S ID='S-12' AZ='CTR'> From a practical perspective , there is ample evidence that limited mixed-initiative has contributed to lack of system usability . </S>
9504007.xml:<S ID='S-13' AZ='CTR'> Many researchers have noted that the absence of mixed-initiative gives rise to two problems with expert systems : They don't allow users to participate in the reasoning process , or to ask the questions they want answered <REF TYPE='P'>Pollack et al. 1982</REF> , <REF  TYPE='P'>Kidd 1985</REF>, <REF  TYPE='P'>Frohlich and Luff 1989</REF> . </S>
9504007.xml:<S ID='S-14' AZ='CTR'> In addition , question answering systems often fail to take account of the system 's role as a conversational partner . </S>
9504007.xml:<S ID='S-15' AZ='CTR'> For example , fragmentary utterances may be interpreted with respect to the previous user input , but what users say is often in reaction to the system 's previous response <REF  TYPE='P'>Cohen and Perrault 1982</REF>, <REF  TYPE='P'>Sidner 1983</REF> . </S>
9504007.xml:<S ID='S-185' AZ='CTR'> <REFAUTHOR>Hobbs</REFAUTHOR> has some difficulties determining the function of this repetition , but we maintain that the function follows from the more general principles of the control rules : speakers signal that they wish to shift control by supplying no new propositional content . </S>
9504017.xml:<S ID='S-12' AZ='CTR'> The problem with these approaches is that they assign a dual life to pragmatic inferences : in the initial stage , as members of a simple or complex utterance , they are defeasible . </S>
9504017.xml:<S ID='S-13' AZ='CTR'> However , after that utterance is analyzed , there is no possibility left of cancelling that inference . </S>
9504017.xml:<S ID='S-14' AZ='CTR'> But it is natural to have implicatures and presuppositions that are inferred and cancelled as a sequence of utterances proceeds : research in conversation repairs <REF TYPE='P' SELF="YES">Hirst et al. 1994</REF> abounds in such examples . </S>
9504017.xml:<S ID='S-17' AZ='CTR'> <REF TYPE='A'>Green 1992</REF> assumes that implicatures are connected to discourse entities and not to utterances , but her approach still does not allow cancellations across discourse units . </S>
9504017.xml:<S ID='S-19' AZ='CTR'> <REF TYPE='A'>Mercer 1987</REF> formalizes presuppositions in a logical framework that handles defaults <REF TYPE='P'>Reiter 1980</REF> , but this approach is not tractable and it treats natural disjunction as an exclusive-or and implication as logical equivalence . </S>
9504017.xml:<S ID='S-20' AZ='CTR'> Computational approaches fail to account for the cancellation of pragmatic inferences : once presuppositions <REF TYPE='P'>Weischedel 1979</REF> or implicatures <REF TYPE='P'>Hirschberg 1985</REF> , <REF TYPE='P'>Green 1992</REF> are generated , they can never be cancelled . </S>
9504017.xml:<S ID='S-21' AZ='CTR'> We are not aware of any formalism or computational approach that offers a unified explanation for the cancellability of pragmatic inferences in general , and of no approach that handles cancellations that occur in sequences of utterances . </S>
9504017.xml:<S ID='S-65' AZ='CTR'> In contrast with most other approaches , we provide a consistent methodology for computing these inferences and for determining whether they are cancelled or not for all possible configurations : simple and complex utterances and sequences of utterances . </S>
9504017.xml:<S ID='S-77' AZ='CTR'> The Achilles heel for most theories of presupposition has been their vulnerability to the projection problem . </S>
9504017.xml:<S ID='S-117' AZ='CTR'> Our approach does not exhibit these constraints . </S>
9504017.xml:<S ID='S-124' AZ='CTR'> Unlike most research in pragmatics that focuses on certain types of presuppositions or implicatures , we provide a global framework in which one can express all these types of pragmatic inferences . </S>
9504026.xml:<S ID='S-54' AZ='CTR'> Note that the construction of <REFAUTHOR>Bar-Hillel</REFAUTHOR> would have yielded a grammar with 88 rules . </S>
9504026.xml:<S ID='S-57' AZ='CTR'> However , if we use that method we will end up ( typically ) with an enormously large forest grammar that is not even guaranteed to contain solutions . </S>
9504026.xml:<S ID='S-63' AZ='CTR'> But if we use existing techniques for parsing DCGs , then we are also confronted with an undecidability problem : the recognition problem for DCGs is undecidable <REF TYPE='P'>Pereira and Warren 1983</REF> . </S>
9504026.xml:<S ID='S-64' AZ='CTR'> A fortiori the problem of deciding whether the intersection of a FSA and a DCG is empty or not is undecidable . </S>
9504026.xml:<S ID='S-70' AZ='CTR'> This observation is not very helpful in establishing insights concerning interesting subclasses of DCGs for which termination can be guaranteed ( in the case of FSA input ) . </S>
9504026.xml:<S ID='S-71' AZ='CTR'> The reason is that there are now two sources of recursion : in the DCG and in the FSA ( cycles ) . </S>
9504026.xml:<S ID='S-72' AZ='CTR'> As we saw earlier : even for CFG it holds that there can be an infinite number of analyses for a given FSA ( but in the CFG this of course does not imply undecidability ) . </S>
9504027.xml:<A-S ID='A-1' DOCUMENTC='S-16' AZ='CTR'> However , the Shake-and-Bake generation algorithm of <REF TYPE='A' SELF="YES">Whitelock 1992</REF> is NP-complete . </A-S>
9504027.xml:<S ID='S-1' AZ='CTR'> Unfortunately , the generation algorithms described to date have been intractable . </S>
9504027.xml:<S ID='S-16' ABSTRACTC='A-1' AZ='CTR'> However , none of these approaches is guaranteed to avoid protracted search times if an exact answer is required , because bag generation is NP-complete <REF TYPE='P'>Brew 1992</REF> . </S>
9504027.xml:<S ID='S-28' AZ='CTR'> However , the target derivation information itself is not used to assist the algorithm . </S>
9504027.xml:<S ID='S-29' AZ='CTR'> Even in <REF TYPE='A' SELF="YES">Beaven 1992a</REF> , the derivation information is used simply to cache previous results to avoid exact recomputation at a later stage , not to improve on previous guesses . </S>
9504027.xml:<S ID='S-35' AZ='CTR'> Generation will fail if all signs in the bag are not eventually incorporated in the final result , but in the naive algorithm , the intervening computation may be intractable . </S>
9504027.xml:<S ID='S-173' AZ='CTR'> We tested a TNCB-based generator in the SLEMaT MT system with the pathological cases described in <REF TYPE='A'>Brew 1992</REF> against <REFAUTHOR SELF="YES">Whitelock</REFAUTHOR> 's original generation algorithm , and have obtained speed improvements of several orders of magnitude . </S>
9504030.xml:<A-S ID='A-0' AZ='CTR'> Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general . </A-S>
9504030.xml:<A-S ID='A-6' DOCUMENTC='S-20' AZ='CTR'> In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser . </A-S>
9504030.xml:<S ID='S-2' AZ='CTR'> However , these approaches have proved too brittle for most interesting natural language problems . </S>
9504030.xml:<S ID='S-15' AZ='CTR'> One of the important points of this work is that statistical models of natural language should not be restricted to simple , context-insensitive models . </S>
9504030.xml:<S ID='S-16' AZ='CTR'> In a problem like parsing , where long-distance lexical information is crucial to disambiguate interpretations accurately , local models like probabilistic context-free grammars are inadequate . </S>
9504030.xml:<S ID='S-80' AZ='CTR'> Unfortunately , they assign probability zero to events which can possibly occur . </S>
9504030.xml:<S ID='S-155' AZ='CTR'> After over ten years of grammar development , the IBM parser achieved a 0-crossing - brackets score of 69 % . </S>
9504030.xml:<S ID='S-156' AZ='CTR'> On this same test set , SPATTER scored 76 % . </S>
9504030.xml:<S ID='S-186' AZ='CTR'> Statistical models for parsing need to consider many more features of a sentence than can be managed by n-gram modeling techniques and many more examples than a human can keep track of . </S>
9504033.xml:<A-S ID='A-3' DOCUMENTC='S-186' AZ='CTR'> Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . </A-S>
9504033.xml:<S ID='S-4' AZ='CTR'> While substantial work on noun compounds exists in both linguistics <REF TYPE='P'>Levi 1978</REF> , <REF TYPE='P'>Ryder 1994</REF> and computational linguistics <REF TYPE='P'>Finin 1980</REF> , <REF TYPE='P'>McDonald 1982</REF> , <REF TYPE='P'>Isabelle 1984</REF> , techniques suitable for broad coverage parsing remain unavailable . </S>
9504033.xml:<S ID='S-18' AZ='CTR'> While <REF TYPE='A'>Hindle and Rooth 1993</REF> use a partial parser to acquire training data , such machinery appears unnecessary for noun compounds . </S>
9504033.xml:<S ID='S-29' AZ='CTR'> Only two of these have been subjected to evaluation , and in each case , no comparison to any of the other three was performed . </S>
9504033.xml:<S ID='S-30' AZ='CTR'> In fact all authors appear unaware of the other three proposals . </S>
9504033.xml:<S ID='S-44' AZ='CTR'> Since this is proposed merely as a rough heuristic , it is not stated what the outcome is to be if neither or both subcomponents appear . </S>
9504033.xml:<S ID='S-45' AZ='CTR'> Nor is there any evaluation of the algorithm . </S>
9504033.xml:<S ID='S-48' AZ='CTR'> Again , there is no evaluation of the method other than a demonstration that four examples work correctly . </S>
9504033.xml:<S ID='S-75' AZ='CTR'> In contrast , the adjacency model appears to predict a proportion of 50 . </S>
9504033.xml:<S ID='S-78' AZ='CTR'> A simple calculation shows that using their own preprocessing hueristics to guess a bracketing provides a higher accuracy on their test set than their statistical model does . </S>
9504033.xml:<S ID='S-79' AZ='CTR'> This renders their experiment inconclusive . </S>
9504033.xml:<S ID='S-130' AZ='CTR'> The equations presented above for the dependency model differ from those developed in <REF TYPE='A' SELF="YES">Lauer and Dras 1994</REF> in one way . </S>
9504033.xml:<S ID='S-131' AZ='CTR'> There , an additional weighting factor ( of 2.0 ) is used to favour a left-branching analysis . </S>
9504033.xml:<S ID='S-132' AZ='CTR'> This arises because their construction is based on the dependency model which predicts that left-branching analyses should occur twice as often . </S>
9504033.xml:<S ID='S-133' AZ='CTR'> Also , the work reported in <REF TYPE='A' SELF="YES">Lauer and Dras 1994</REF> uses simplistic estimates of the probability of a word given its thesaurus category . </S>
9504033.xml:<S ID='S-134' AZ='CTR'> The equations above assume these probabilities are uniformly constant . </S>
9504033.xml:<S ID='S-141' AZ='CTR'> As can be seen , the dependency model is more accurate than the adjacency model . </S>
9504033.xml:<S ID='S-147' AZ='CTR'> In the case of the pattern training scheme , the difference between 68.9 % for adjacency and 77.5 % for dependency is statistically significant at the 5 % level ( p = 0.0316 ) , demonstrating the superiority of the dependency model , at least for the compounds within Grolier 's encyclopedia . </S>
9504033.xml:<S ID='S-183' AZ='CTR'> This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed . </S>
9504033.xml:<S ID='S-184' AZ='CTR'> In this study , not only has the technique proved its worth by supporting generality , but through generalisation of training information it outperforms the equivalent lexical association approach given the same information . </S>
9504033.xml:<S ID='S-186' ABSTRACTC='A-3' AZ='CTR'> In all experiments the dependency model provides a substantial advantage over the adjacency model , even though the latter is more prevalent in proposals within the literature . </S>
9504034.xml:<A-S ID='A-2' DOCUMENTC='S-115' AZ='CTR'> We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks . </A-S>
9504034.xml:<A-S ID='A-3' AZ='CTR'> In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques . </A-S>
9504034.xml:<A-S ID='A-4' AZ='CTR'> The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . </A-S>
9504034.xml:<S ID='S-2' AZ='CTR'> Yet , n-gram language models can only capture dependencies within an n-word window , where currently the largest practical n for natural language is three , and many dependencies in natural language occur beyond a three-word window . </S>
9504034.xml:<S ID='S-3' AZ='CTR'> In addition , n-gram models are extremely large , thus making them difficult to implement efficiently in memory-constrained applications . </S>
9504034.xml:<S ID='S-6' AZ='CTR'> However , to date there has been little success in constructing grammar-based language models competitive with n-gram models in problems of any magnitude . </S>
9504034.xml:<S ID='S-8' AZ='CTR'> This result marks the first time a grammar-based language model has surpassed n-gram modeling in a task of at least moderate size . </S>
9504034.xml:<S ID='S-14' AZ='CTR'> However , the optimal grammar under this objective function is one which generates only strings in the training data and no other strings . </S>
9504034.xml:<S ID='S-15' AZ='CTR'> Such grammars are poor language models , as they overfit the training data and do not model the language at large . </S>
9504034.xml:<S ID='S-17' AZ='CTR'> However , in our work we do not wish to limit the size of the grammars considered . </S>
9504034.xml:<S ID='S-18' AZ='CTR'> The basic shortcoming of the maximum-likelihood objective function is that it does not encompass the compelling intuition behind Occam 's Razor , that simpler ( or smaller ) grammars are preferable over complex ( or larger ) grammars . </S>
9504034.xml:<S ID='S-108' AZ='CTR'> However , <REFAUTHOR>Solomonoff</REFAUTHOR> does not specify a concrete search algorithm and only makes suggestions as to its nature . </S>
9504034.xml:<S ID='S-111' AZ='CTR'> However , a different prior probability on grammars is used , and the algorithms are only efficient enough to be applied to small data sets . </S>
9504034.xml:<S ID='S-114' AZ='CTR'> To our knowledge , neither algorithm has surpassed the performance of n-gram models in a language modeling task of substantial scale . </S>
9504034.xml:<S ID='S-115' ABSTRACTC='A-2' AZ='CTR'> To evaluate our algorithm , we compare the performance of our algorithm to that of n-gram models and the Inside-Outside algorithm . </S>
9504034.xml:<S ID='S-150' AZ='CTR'> We achieve a moderate but significant improvement in performance over n-gram models and the Inside-Outside algorithm in the first two domains , while in the part-of-speech domain we are outperformed by n-gram models but we vastly outperform the Inside-Outside algorithm . </S>
9504034.xml:<S ID='S-154' AZ='CTR'> Notice that our algorithm produces a significantly more compact model than the n-gram model , while running significantly faster than the Inside-Outside algorithm even though we use an Inside-Outside post-pass . </S>
9504034.xml:<S ID='S-156' AZ='CTR'> Our algorithm consistently outperformed the Inside-Outside algorithm in these experiments . </S>
9504034.xml:<S ID='S-163' AZ='CTR'> Hence , our algorithm should be less prone to suboptimal local minima than the Inside-Outside algorithm . </S>
9504034.xml:<S ID='S-164' AZ='CTR'> Outperforming n-gram models in the first two domains demonstrates that our algorithm is able to take advantage of the grammatical structure present in data . </S>
9504034.xml:<S ID='S-165' AZ='CTR'> However , the superiority of n-gram models in the part-of-speech domain indicates that to be competitive in modeling naturally-occurring data , it is necessary to model collocational information accurately . </S>
9505001.xml:<S ID='S-12' AZ='CTR'> Researchers have studied the analysis and generation of arguments <REF TYPE='P'>Birnbaum et al. 1980</REF> , <REF TYPE='P'>Reichman 1981</REF> , <REF TYPE='P'>Cohen 1987</REF> , <REF TYPE='P'>Sycara 1989</REF> , <REF TYPE='P'>Quilici 1992</REF> , <REF TYPE='P'>Maybury 1993</REF> ; however , agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents . </S>
9505001.xml:<S ID='S-13' AZ='CTR'> <REF TYPE='A'>Sidner 1992</REF> , <REF TYPE='A'>Sidner 1994</REF> formulated an artificial language for modeling collaborative discourse using proposal / acceptance and proposal / rejection sequences ; however , her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions . </S>
9505001.xml:<S ID='S-15' AZ='CTR'> They identified strategies that a system can adopt in justifying its beliefs ; however , they did not specify the criteria under which each of these strategies should be selected . </S>
9505001.xml:<S ID='S-17' AZ='CTR'> However , her model focuses on determining when to include informationally redundant utterances , whereas our model determines whether or not justification is needed for a claim to be convincing and , if so , selects appropriate evidence from the system 's private beliefs to support the claim . </S>
9505001.xml:<S ID='S-20' AZ='CTR'> However , our analysis of naturally-occurring consultation dialogues <REF TYPE='P'>Columbia University Transcripts 1985</REF> , <REF TYPE='P'>SRI Transcripts 1992</REF> shows that in other domains conflict resolution does extend beyond a single exchange of conflicting beliefs ; therefore we employ a recursive model for collaboration that captures extended negotiation and represents the structure of the discourse . </S>
9505001.xml:<S ID='S-21' AZ='CTR'> Furthermore , their system deals with a single conflict , while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise . </S>
9505001.xml:<S ID='S-27' AZ='CTR'> This differentiates collaborative negotiation from argumentation <REF TYPE='P'>Birnbaum et al. 1980</REF> , <REF TYPE='P'>Reichman 1981</REF> , <REF  TYPE='P'>Cohen 1987</REF>, <REF  TYPE='P'>Quilici 1992</REF> . </S>
9505001.xml:<S ID='S-29' AZ='CTR'> This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation <REF TYPE='P'>Sycara 1989</REF> . </S>
9505001.xml:<S ID='S-31' AZ='CTR'> Although agents involved in argumentation and non-collaborative negotiation take other agents ' beliefs into consideration , they do so mainly to find weak points in their opponents ' beliefs and attack them to win the argument . </S>
9505001.xml:<S ID='S-35' AZ='CTR'> However , our research did not specify , in cases where multiple conflicts arise , how an agent should identify which part of an unaccepted proposal to address or how to select evidence to support the proposed modification . </S>
9506004.xml:<A-S ID='A-2' DOCUMENTC='S-2' AZ='CTR'> However , for some interesting cases , such as a Combinatory Categorial Grammar account of coordination constructs , this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation . </A-S>
9506004.xml:<S ID='S-2' ABSTRACTC='A-2' AZ='CTR'> However , there are cases in which this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation . </S>
9506004.xml:<S ID='S-4' AZ='CTR'> While many aspects of CCG semantics can be reasonably simulated in first-order unification , the simulation breaks down on some of the most interesting cases that CCG can theoretically handle . </S>
9506004.xml:<S ID='S-5' AZ='CTR'> The problem in general , and for CCG in particular , is that the implementation language does not have sufficient expressive power to allow a more direct encoding . </S>
9506004.xml:<S ID='S-7' AZ='CTR'> We begin by briefly illustrating why first-order unification is inadequate for some coordination constructs , and then review two proposed solutions . </S>
9506004.xml:<S ID='S-12' AZ='CTR'> However , under first-order unification , this needs to simulated by having the variable x in <EQN/> unify both with Bill and John , and this is not possible . </S>
9506004.xml:<S ID='S-18' AZ='CTR'> However , as the semantic terms become more complex , it is no trivial matter to write <EQN/> - reduction that will correctly handle variable capture . </S>
9506004.xml:<S ID='S-19' AZ='CTR'> Also , if at some point it was desired to determine if the semantic forms of two different sentences were the same , a predicate would be needed to compare two lambda forms for <EQN/> - equivalence , which again is not a simple task . </S>
9506004.xml:<S ID='S-20' AZ='CTR'> Essentially , the logic variable X is meant to be interpreted as a bound variable , which requires an additional layer of programming . </S>
9506004.xml:<S ID='S-24' AZ='CTR'> While this pushes first-order unification beyond what it had been previously shown capable of , there are two disadvantages to this technique : </S>
9506004.xml:<S ID='S-25' TYPE='ITEM' AZ='CTR' > For every possible category that can be conjoined , a separate lexical entry for and is required , and </S>
9506004.xml:<S ID='S-26' TYPE='ITEM' AZ='CTR' > As the conjoinable categories become more complex , the and entries become correspondingly more complex and greatly obscure the theoretical background of the grammar formalism . </S>
9506004.xml:<S ID='S-27' AZ='CTR'> The fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by Prolog , but instead needs to be implemented by additional programming . </S>
9506004.xml:<S ID='S-28' AZ='CTR'> While theoretically possible , it becomes quite problematic to actually implement . </S>
9511001.xml:<S ID='S-8' AZ='CTR'> Recently , <REF TYPE='A'>Murata and Nagao 1993</REF> have proposed a method of determining the referentiality property and number of nouns in Japanese sentences for machine translation into English , but the research has not yet been extended to include the actual English generation . </S>
9511006.xml:<A-S ID='A-1' DOCUMENTC='S-1' AZ='CTR'> However , for many tasks , one is interested in relationships among word senses , not words . </A-S>
9511006.xml:<S ID='S-1' ABSTRACTC='A-1' AZ='CTR'> However , for many tasks , one is interested in relationships among word senses , not words . </S>
9511006.xml:<S ID='S-4' AZ='CTR'> Yet a computational system has no choice but to consider other , more awkward possibilities -- for example , this cluster might be capturing a distributional relationship between advice ( as one sense of counsel ) and royalty ( as one sense of court ) . </S>
9511006.xml:<S ID='S-5' AZ='CTR'> This would be a mistake for many applications , such as query expansion in information retrieval , where a surfeit of false connections can outweigh the benefits obtained by using lexical knowledge . </S>
9511006.xml:<S ID='S-8' AZ='CTR'> Unfortunately , there are few corpora annotated with word sense information , and computing reliable statistics on word senses rather than words will require more data , rather than less . </S>
9511006.xml:<S ID='S-9' AZ='CTR'> Furthermore , one widely available example of a large , manually sense-tagged corpus -- the WordNet group 's annotated subset of the Brown corpus -- vividly illustrates the difficulty in obtaining suitable data . </S>
9511006.xml:<S ID='S-10' AZ='CTR'> It is quite small , by current corpus standards ( on the order of hundreds of thousands of words , rather than millions or tens of millions ) ; the direct annotation methodology used to create it is labor intensive ( <REF TYPE='A'>Marcus et al. 1993</REF> found that direct annotation takes twice as long as automatic tagging plus correction , for part-of-speech annotation ) ; and the output quality reflects the difficulty of the task ( inter-annotator disagreement is on the order of 10 % , as contrasted with the approximately 3 % error rate reported for part-of-speech annotation by <REFAUTHOR>Marcus et al.</REFAUTHOR> ) . </S>
9511006.xml:<S ID='S-16' AZ='CTR'> This paper begins from a rather different starting point . </S>
9511006.xml:<S ID='S-37' AZ='CTR'> However , there are problems with the simple path-length definition of semantic similarity , and experiments using WordNet show that other measures of semantic similarity , such as the one employed here , provide a better match to human similarity judgments than simple path length does <REF SELF="YES" TYPE='P'>Resnik 1995a</REF> . </S>
9511006.xml:<S ID='S-77' AZ='CTR'> Here I make an explicit comparison with <REFAUTHOR>Sussna</REFAUTHOR> 's approach , since it is the most similar of previous work . </S>
9511006.xml:<S ID='S-82' AZ='CTR'> However , there are some important differences , as well . </S>
9511006.xml:<S ID='S-83' AZ='CTR'> First , unlike <REFAUTHOR>Sussna</REFAUTHOR> 's proposal , this algorithm aims to disambiguate groupings of nouns already established ( e.g. by clustering , or by manual effort ) to be related , as opposed to groupings of nouns that happen to appear near each other in running text ( which may or may not reflect relatedness based on meaning ) . </S>
9511006.xml:<S ID='S-85' AZ='CTR'> Second , this difference is reflected algorithmically by the fact that <REFAUTHOR>Sussna</REFAUTHOR> uses not only links but also other WordNet links such as PART-OF . </S>
9511006.xml:<S ID='S-86' AZ='CTR'> Third , unlike <REFAUTHOR>Sussna</REFAUTHOR> 's algorithm , the semantic similarity / distance computation here is not based on path length , but on information content , a choice that I have argued for elsewhere <REF  SELF="YES" TYPE='P'>Resnik 1993</REF>, <REF  SELF="YES" TYPE='P'>Resnik 1995a</REF> . </S>
9511006.xml:<S ID='S-87' AZ='CTR'> Fourth , the combinatorics are handled differently : <REFAUTHOR>Sussna</REFAUTHOR> explores analyzing all sense combinations ( and living with the exponential complexity ) , as well as the alternative of sequentially `` freezing '' a single sense for each of <EQN/> and using those choices , assumed to be correct , as the basis for disambiguating <EQN/> . </S>
9511006.xml:<S ID='S-88' AZ='CTR'> The algorithm presented here falls between those two alternatives . </S>
9511006.xml:<S ID='S-89' ABSTRACTC='A-3' AZ='CTR'> A final , important difference between this algorithm and previous algorithms for sense disambiguation is that it offers the possibility of assigning higher-level WordNet categories rather than lowest-level sense labels . </S>
9511006.xml:<S ID='S-147' AZ='CTR'> The results of the evaluation are extremely encouraging , especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs <REF  TYPE='P'>Hearst 1991</REF>, <REF  TYPE='P'>Cowie et al. 1992</REF> . </S>
9511006.xml:<S ID='S-157' AZ='CTR'> The difficulty with the latter kind of knowledge is that , until now , the widespread success in characterizing lexical behavior in terms of distributional relationships has applied at the level of words -- indeed , word forms -- as opposed to senses . </S>
9601004.xml:<S ID='S-32' AZ='CTR'> However , the following problems arise from the semantic differential procedure as measurement of meaning . </S>
9601004.xml:<S ID='S-33' AZ='CTR'> The procedure is not based on the denotative meaning of a word , but only on the connotative emotions attached to the word ; it is difficult to choose the relevant dimensions , i.e. the dimensions required for the sufficient semantic space . </S>
9601004.xml:<S ID='S-37' AZ='CTR'> However , thesauri provide neither information about semantic difference between words juxtaposed in a category , nor about strength of the semantic relation between words -- both are to be dealt in this paper . </S>
9601004.xml:<S ID='S-145' AZ='CTR'> <REFAUTHOR>Osgood</REFAUTHOR> 's semantic differential procedure used 50 adjective dimensions ; our semantic measurement uses 2,851 dimensions with completeness and objectivity . </S>
9604019.xml:<S ID='S-2' AZ='CTR'> At the same time it often remains unclear how these optimizations relate to each other and what they actually mean . </S>
9604019.xml:<S ID='S-12' AZ='CTR'> This can lead to nontermination as the tree fragments enumerated in bottom-up evaluation of magic compiled grammars are connected <REF TYPE='P'>Johnson forthcoming</REF> . </S>
9604019.xml:<S ID='S-13' AZ='CTR'> More specifically , 'magic generation ' falls prey to non-termination in the face of head recursion , i.e. , the generation analog of left recursion in parsing . </S>
9604019.xml:<S ID='S-21' AZ='CTR'> Even though these approaches often accomplish considerable improvements with respect to efficiency or termination behavior , it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering . </S>
9604019.xml:<S ID='S-76' AZ='CTR'> Just like in case of <REFAUTHOR>Shieber</REFAUTHOR> 's bottom-up generator , bottom-up evaluation of magic-compiled grammars produced with this Magic variant is only guaranteed to be complete in case the original grammar obeys the semantic monotonicity constraint . </S>
9604019.xml:<S ID='S-80' AZ='CTR'> Just like top-down evaluation of the original grammar bottom-up evaluation of its magic compiled version falls prey to non-termination in the face of head recursion . </S>
9604022.xml:<A-S ID='A-0' DOCUMENTC='S-0' AZ='CTR'> Words unknown to the lexicon present a substantial problem to part-of-speech tagging . </A-S>
9604022.xml:<A-S ID='A-3' AZ='CTR'> The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . </A-S>
9604022.xml:<S ID='S-11' AZ='CTR'> A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources . </S>
9604022.xml:<S ID='S-17' AZ='CTR'> Although the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art , they require specially prepared training data -- a pre-tagged training corpus , training examples , etc . </S>
9604022.xml:<S ID='S-25' AZ='CTR'> Unlike other approaches we don't require the corpus to be pre-annotated but use it in its raw form . </S>
9604022.xml:<S ID='S-92' AZ='CTR'> The main difference between the two functions is that there the z value was implicitly assumed to be 1 which corresponds to the confidence of 68 % . </S>
9604022.xml:<S ID='S-134' AZ='CTR'> In comparison with the Xerox word-ending guesser taken as the base-line model we detect a substantial increase in the precision by about 22 % and a cheerful increase in coverage by about 6 % . </S>
9604022.xml:<S ID='S-135' AZ='CTR'> This means that the Xerox guesser creates more ambiguity for the disambiguator , assigning five instead of three POS s in the example above . </S>
9604022.xml:<S ID='S-136' AZ='CTR'> It can also handle 6 % less unknown words which , in fact , might decrease its performance even lower . </S>
9604022.xml:<S ID='S-157' AZ='CTR'> In this evaluation we tried two different taggers . </S>
9604022.xml:<S ID='S-184' AZ='CTR'> When using the Xerox tagger with its original guesser , 63 unknown words were incorrectly tagged and the accuracy on the unknown words was measured at 81.8 % . </S>
9604022.xml:<S ID='S-185' AZ='CTR'> When the Xerox tagger was equipped with our cascading guesser its accuracy on unknown words increased by almost 9 % upto 90.5 % . </S>
9604022.xml:<S ID='S-187' AZ='CTR'> The cascading guesser performed better than <REFAUTHOR>Brill</REFAUTHOR> 's original guesser by about 8 % boosting the performance on the unknown words from 84.5 % to 92.2 % . </S>
9604022.xml:<S ID='S-196' AZ='CTR'> The best results on unknown words were again obtained on the cascading guesser ( 86 % - 87.45 % ) and <REFAUTHOR>Brill</REFAUTHOR> 's tagger again did better then the Xerox one by 1.5 . </S>
9604022.xml:<S ID='S-206' AZ='CTR'> Evaluation of tagging accuracy on unknown words using texts unseen by the guessers and the taggers at the training phase showed that tagging with the automatically induced cascading guesser was consistently more accurate than previously quoted results known to the author ( 85 % ) . </S>
9604022.xml:<S ID='S-207' AZ='CTR'> The cascading guesser outperformed the guesser supplied with the Xerox tagger by about 8 - 9 % and the guesser supplied with <REFAUTHOR>Brill</REFAUTHOR> 's tagger by about 6 - 7 % . </S>
9605013.xml:<S ID='S-12' AZ='CTR'> There has been no method proposed to date , however , that learns dependencies between case slots in the natural language processing literature . </S>
9605013.xml:<S ID='S-60' AZ='CTR'> As illustrated in Section <CREF/> , this assumption is not necessarily valid in practice . </S>
9605013.xml:<S ID='S-130' AZ='CTR'> We see that using the information on dependency we can significantly improve the disambiguation accuracy on this part of the data . </S>
9605014.xml:<A-S ID='A-2' DOCUMENTC='S-10;S-11' AZ='CTR'> We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter . </A-S>
9605014.xml:<S ID='S-10' ABSTRACTC='A-2' AZ='CTR'> In particular , we compared the performance of an MDL-based simulated annealing algorithm in hierarchical word clustering against that of one based on the Maximum Likelihood Estimator ( MLE , for short ) . </S>
9605014.xml:<S ID='S-11' ABSTRACTC='A-2' AZ='CTR'> We found that the MDL-based method performs better than the MLE-based method . </S>
9605014.xml:<S ID='S-80' AZ='CTR'> Here , we restrict our attention on ` hard clustering ' ( i.e. , each word must belong to exactly one class ) , in part because we are interested in comparing the thesauri constructed by our method with existing hand-made thesauri . </S>
9605014.xml:<S ID='S-86' AZ='CTR'> There is no guarantee , however , that the employed smoothing method is in any way consistent with the clustering method used subsequently . </S>
9605014.xml:<S ID='S-95' AZ='CTR'> In the presence of models with varying complexity , MLE tends to overfit the data , and output a model that is too complex and tailored to fit the specifics of the input data . </S>
9605014.xml:<S ID='S-96' AZ='CTR'> If we employ MLE as criterion in our simulated annealing algorithm , it will result in selecting a very fine model with many small clusters , most of which will have probabilities estimated as zero . </S>
9605014.xml:<S ID='S-97' AZ='CTR'> Thus , in contrast to employing MDL , it will not have the effect of smoothing at all . </S>
9605014.xml:<S ID='S-98' AZ='CTR'> Purely as a method of estimation as well , the superiority of MDL over MLE is supported by convincing theoretical findings <REF TYPE='P'>Barron and Cover 1991</REF> , <REF TYPE='P'>Yamanishi 1992</REF> . </S>
9605014.xml:<S ID='S-100' AZ='CTR'> ( The models selected by MDL converge to the true model approximately at the rate of 1 / s where s is the number of parameters in the true model , whereas for MLE the rate is 1 / t , where t is the size of the domain , or in our context , the total number of elements of <EQN/> . ) </S>
9605014.xml:<S ID='S-101' AZ='CTR'> ` Consistency ' is another desirable property of MDL , which is not shared by MLE . </S>
9605014.xml:<S ID='S-104' AZ='CTR'> In particular , we have compared the performance of employing an MDL-based simulated annealing against that of one based on MLE in word clustering . </S>
9605014.xml:<S ID='S-113' AZ='CTR'> The results indicate that MDL converges to the true model faster than MLE . </S>
9605014.xml:<S ID='S-114' AZ='CTR'> Also , MLE tends to select a model overfitting the data , while MDL tends to select a model which is simple and yet fits the data reasonably well . </S>
9605014.xml:<S ID='S-117' AZ='CTR'> We conclude that it is better to employ MDL than MLE in word clustering . </S>
9605014.xml:<S ID='S-141' AZ='CTR'> Note that the coverage of ` MDL-Thesaurus ' significantly outperformed that of ` Word-Based , ' while basically maintaining high accuracy ( though it drops somewhat ) , indicating that using an automatically constructed thesaurus can improve disambiguation results in terms of coverage . </S>
9605014.xml:<S ID='S-142' AZ='CTR'> We also tested the method proposed in <REF TYPE='A' SELF="YES">Li and Abe 1995</REF> of learning case frames patterns using an existing thesaurus . </S>
9605014.xml:<S ID='S-155' AZ='CTR'> Our best disambiguation result obtained using this last combined method somewhat improves the accuracy reported in <REF TYPE='A' SELF="YES">Li and Abe 1995</REF> ( <EQN/> ) . </S>
9605014.xml:<S ID='S-159' AZ='CTR'> Our experimental results show that it is better to employ MDL than MLE as estimation criterion in word clustering . </S>
